\documentclass{article}
\usepackage[utf8x]{inputenc}
\usepackage[english,russian]{babel}
\usepackage{amsmath,amsthm,amssymb}

% Rich title
\usepackage{titling}

% Matrix operations
\usepackage{gauss}

\usepackage{pgfplots}
\usetikzlibrary{patterns}

% Russian traditions
\renewcommand{\epsilon}{\varepsilon}
\renewcommand{\phi}{\varphi}
\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}
\newcommand{\intl}{\int\limits}
\usepackage{misccorr}

% Bib in TOC
\usepackage[numbib,nottoc]{tocbibind}

\usepackage{tocbibind}

% Custom commands
\providecommand{\program}[1]{{\tt #1}}
\providecommand{\La}{\mathcal{L}}
\providecommand{\neword}{\emph}
\providecommand{\pardiff}[2]{\frac{\partial{#1}}{\partial{#2}}}
\providecommand{\dpardiff}[3]{\frac{\partial^2{#1}}{\partial{#2}\partial{#3}}}
\providecommand{\abs}[1]{\left \lvert{#1}\right \rvert}
\providecommand{\norm}[1]{\left \lVert{#1}\right \rVert}
\providecommand{\set}[1]{\mathbb{#1}}
\newcommand{\scalmult}[1]{{\left \langle #1 \right \rangle}}
\DeclareMathOperator{\extr}{extr}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\Sp}{Sp}

\newtheorem{thm}{Теорема}[section]
\theoremstyle{remark}
\newtheorem{rem}{Замечание}[section]
\theoremstyle{definition}
\newtheorem{dfn}{Определение}[section]

\numberwithin{equation}{section}

\usepackage[unicode,
pdftex, colorlinks, linkcolor=blue, citecolor=teal,
pdfauthor=Dmitry Dzhus]{hyperref}

\begin{document}
\author{Дмитрий Джус}
\title{Курсовая работа по теме \\
  \Huge{«Методы оптимизации»}}
\pretitle{\begin{center}\LARGE}
\posttitle{\par\end{center}\vskip 3pc}
\date{}
\maketitle
\thispagestyle{empty}

\clearpage
\tableofcontents

\clearpage
\section{Предмет работы}

Настоящая курсовая работа посвящена методам оптимизации и
экстремальным задачам.

В первой части рассмотрено решение задачи условной минимизации
линейной функции с помощью симплекс-метода.

Кроме того, дано описание градиентного метода многопараметрической
оптимизации с чебышёвским функциями релаксации, предложена реализация
метода на алгоритмическом языке и представлены результаты работы на
тестовых функциях.

\clearpage
\section{Симплекс-метод}
\label{sec:simplex}

Симплекс-метод позволяет решать задачи линейного программирования,
заключающиеся в минимизации целевого линейного функционала при
заданных линейных ограничениях. Основной идеей симплекс-метода
является перебор вершин выпуклого многогранника в многомерном
пространстве. Описание и общую схему метода можно найти в
\cite{taha05}.

\subsection{Практический пример}

К решению предлагается следующая задача:
\begin{equation}
  \label{eq:lp-initial}
  \begin{cases}
    f(x) = -3x_1+2x_2-2x_3+2x_4-x_5 \to \min \\
    -x_1+x_2-x_3=1 \\
    -x_2+x_3+x_4=1 \\
    \phantom{-}x_2+x_3+x_5=2 \\
    \phantom{-}x_i \geq 0,\, i = \overline{1, 5}
  \end{cases}
\end{equation}

Для того, чтобы матрица коэффициентов ограничений содержала в себе
единичную, воспользуемся методом искусственных переменных, добавив в
первое ограничение переменную $x_6$ так, что оно примет вид
\begin{equation*}
  -x_1+x_2-x_3+x_6=1 \\
\end{equation*}

Заменим минимизируемую функцию на $\tilde{f}(x) = x_6$ и выразим её
через свободные переменные $x_1, x_2, x_3$, получив
\mbox{$\tilde{f}(x) = x_1-x_2+x_3+1$}. Перейдём к рассмотрению
вспомогательной задачи
\begin{equation}
  \begin{cases}
    \tilde{f}(x) = x_1-x_2+x_3+1 \to \min \\
    -x_1+x_2-x_3+x_6=1 \\
    -x_2+x_3+x_4=1 \\
    \phantom{-}x_2+x_3+x_5=2 \\
    \phantom{-}x_i \geq 0,\, i = \overline{1, 6}
  \end{cases}
\end{equation}

Решим её симплекс-методом. Составим расширенную матрицу из
коэффициентов ограничений и целевой функции:
\begin{equation}
  \begin{gmatrix}[b]
    \dagger & x_1 & x_2 & x_3 & x_4 & x_5 & x_6 & \diamond\\
    x_6 & -1 &  1 & -1 & 0 & 0 & 1 & \mathbf{1} \\
    x_4 &  0 & -1 &  1 & 1 & 0 & 0 & 1 \\
    x_5 &  0 &  1 &  1 & 0 & 1 & 0 & 2 \\
    \tilde{f}    & -1 &  \mathbf{1} & -1 & 0 & 0 & 0 & 1
  \end{gmatrix}
\end{equation}

Базисными переменными являются $x_4, x_5, x_6$. Для дальнейшего
решения необходимо исключить искусственную переменную $x_6$ из состава
базисных. 

Выберем в качестве ведущего столбец с максимальным элементом в
последней строке — это столбец переменной $x_2$, поскольку $\max\{1,
-1, 0\} = 1$. С помощью симплексного отношения выберем первую строку в
качестве ведущей, поскольку $\min\left\{\frac{1}{1},
  \frac{2}{1}\right\} = 1$.

Учитывая выбранный ведущий элемент, переведём переменную $x_2$ в
состав базисных взамен $x_6$. Сложим со второй строкой первую, а с
третьей и четвёртой — первую, умноженную на $-1$:
\begin{equation*}
  \begin{gmatrix}[b]
    \dagger & x_1 & x_2 & x_3 & x_4 & x_5 & x_6 & \diamond\\
    x_6 & -1 &  \mathbf{1} & -1 & 0 & 0 & 1 & 1 \\
    x_4 &  0 & -1 &  1 & 1 & 0 & 0 & 1 \\
    x_5 &  0 &  1 &  1 & 0 & 1 & 0 & 2 \\
    \tilde{f} & -1 &  1 & -1 & 0 & 0 & 0 & 1
    \rowops
    \add{1}{2}
    \add[-1]{1}{3}
    \add[-1]{1}{4}
  \end{gmatrix}
\end{equation*}

Получим матрицу с базисными переменными $x_2, x_4, x_5$:
\begin{equation}
  \label{eq:lp-artif}
  \begin{bmatrix}
    \dagger & x_1 & x_2 & x_3 & x_4 & x_5 & x_6 & \diamond\\
    x_2 & -1 &  1 & -1 & 0 & 0 & 1 &  1\\
    x_4 & -1 &  0 &  0 & 1 & 0 & 0 &  2\\
    x_5 &  1 &  0 &  2 & 0 & 1 & 0 &  1\\
    \tilde{f} & 0 &  0 & 0 & 0 & 0 & -1 & 0
  \end{bmatrix}
\end{equation}

При этом значение функции $\tilde{f}(x)$ равно нулю, что свидетельствует
о том, что найденное решение вспомогательной задачи является
допустимым для исходной.

Отбросим искусственную переменную $x_6$ и вернёмся к начальной задаче
\eqref{eq:lp-initial}. Исключим из исходной целевой функции $f(x)$
базисные переменные $x_2, x_4$ и $x_5$, используя ограничения из
матрицы \eqref{eq:lp-artif}:
\begin{equation*}
  \begin{cases}
    -x_1+x_2-x_3=1\\
    -x_1+x_4=2\\
    \phantom{-}x_1+2x_3+x_5 = 1
  \end{cases}
\end{equation*}

Тогда $f(x)$ принимает вид
\begin{equation*}
  f(x) = 2x_1+2x_3+5
\end{equation*}

Используем эту целевую функцию в матрице ограничений \eqref{eq:lp-artif}:
\begin{equation*}
  \begin{bmatrix}
    \dagger & x_1 & x_2 & x_3 & x_4 & x_5 & \diamond\\
    x_2 & -1 & 1 & -1 & 0 & 0 & 1\\
    x_4 & -1 & 0 &  0 & 1 & 0 & 2\\
    x_5 &  1 & 0 &  2 & 0 & 1 & 1\\
    \tilde{f} & -2 & 0 & -2 & 0 & 0 & 5
  \end{bmatrix}
\end{equation*}

В нижнем ряду под столбцами переменных все коэффициенты отрицательны,
что свидетельствует о том, что найдено оптимальное решение:

\begin{equation}
  \begin{cases}
    x_1 = 0\\
    x_2 = 1\\
    x_3 = 0\\
    x_4 = 2\\
    x_5 = 1
  \end{cases}
\end{equation}

\clearpage
\section{Условная оптимизация}
\label{sec:kuhn-tucker}

\subsection{Теоретические сведения}

Сформулируем без доказательства ряд условий, которые используются при
решении задач условной оптимизации нелинейных функционалов. 

Будем рассматривать следующую задачу:
\begin{equation}
  \label{eq:cond-optim-problem-form}
  \begin{cases}
    f(x) \to \extr \\
    g_j(x) \leq 0,\, j=\overline{1,m} \\
    x \in \set{R}^n
  \end{cases}
\end{equation}

\begin{dfn} \neword{Функцией Лагранжа} задачи
  \eqref{eq:cond-optim-problem-form} называют функцию
  \begin{equation}
    \label{eq:lagrange-form}
    \La(x, \lambda_0, \lambda) = \lambda_0 f(x) + \sum_{j=1}^m {\lambda_j g_j(x)}
  \end{equation}
  где $\lambda$ — вектор $\lambda_1, \dotsc, \lambda_m$. При
  $\lambda_0=1$ функция Лагранжа называется \neword{классической}.
\end{dfn}

\begin{dfn}
  \label{dfn:regular}
  Точка $\hat{x}$, являющаяся решением задачи
  \eqref{eq:cond-optim-problem-form} с функцией Лагранжа
  \eqref{eq:lagrange-form} при $\lambda_0 \neq 0$, называется
  \neword{регулярным экстремумом}, а при $\lambda_0 = 0$ —
  \neword{нерегулярным}.
\end{dfn}

\begin{thm}[Куна—Таккера]
  \label{th:kuhn-tucker}
  Пусть точка $\hat{x}$ является решением задачи
  \eqref{eq:cond-optim-problem-form} с функцией Лагранжа
  \eqref{eq:lagrange-form} при соответствующих $\lambda_0$ и
  $\lambda$. Тогда выполнены следующие условия:
  \begin{enumerate}
    \renewcommand{\labelenumi}{\emph{\asbuk{enumi})}}
  \item $\pardiff{\La}{x_i}=0,\, i=\overline{1,n}$ (условие
    стационарности функции Лагранжа)
  \item $\lambda_j \geq 0,\, j=\overline{1,m}$, если $\hat{x}$ — точка
    минимума, и $\lambda_j \leq 0$, если это точка максимума.
  \item $\lambda_j \cdot g_j(x) = 0,\, j=\overline{1,m}$ (условие
    дополняющей нежёсткости)
  \item $g_j(x) \leq 0,\, j=\overline{1,m}$
  \item $\lambda_0^2 + \norm{\lambda}^2 > 0$ (условие нетривиальности решения)
  \end{enumerate}
\end{thm}

\begin{dfn}
  Точка $x^*$, удовлетворяющая условиям теоремы \ref{th:kuhn-tucker},
  называется \neword{условно-стационарной}.
\end{dfn}

Аналогично определению \ref{dfn:regular} введём схожее и для
условно-стационарных точек.
\begin{dfn}
  Условно-стационарную точку $x^*$, для которой условия
  \ref{th:kuhn-tucker} выполнены при $\lambda ≠ 0$, будем называть
  \neword{регулярной} условно-стационарной точкой задачи
  \eqref{eq:cond-optim-problem-form}.
\end{dfn}

При использовании условий \ref{th:kuhn-tucker} для поиска
условно-стационарных точек рассматривают два варианта значений
$\lambda_0$ в функции Лагранажа: $\lambda_0=0$ и $\lambda_0 \neq 0$. В
последнем случае без ограничения общности обычно полагают $\lambda_0 =
1$.

\begin{thm}[Условие регулярности]
  Для того, чтобы в условно-стационарной точке $x^*$ выполнялось
  неравенство $\lambda_0 \neq 0$, достаточно линейной независимости
  градиентов активных в этой точке ограничений $g_k'(x^*),
  g_{k+1}'(x^*), \dotsc, g_{k+l}'(x^*)$.
\end{thm}

Если удаётся показать, что во \emph{всех} допустимых точках задачи
\eqref{eq:cond-optim-problem-form} выполнено условие регулярности,
случай $\lambda_0=0$ можно исключить из рассмотрения. Для этого также
удобно использовать следующее условие.

\begin{thm}[Условие Слейтера]
  \label{th:slater}
  Для $\lambda_0 \neq 0$ в условиях \ref{th:kuhn-tucker} достаточно
  существования такой точки $x_s$, в которой все неравенства
  ограничений выполняются строго: $g_j(x_s)<0, \, j=\overline{1,m}$.
\end{thm}

Приведём замечание, которое говорит об условиях \emph{достаточности}
условий теоремы \ref{th:kuhn-tucker}.

\begin{rem}
  \label{rem:kt-cond}
  Для точек минимума необходимые условия Куна—Таккера становятся
  достаточными в случае, когда целевая функция $f(x)$ и ограниченное
  неравенствами $g_j(x)$ множество \emph{выпуклы}.

  В случае точек максимума достаточность достигается при
  \emph{вогнутости} функции $f(x)$ и \emph{выпуклости} множества
  допустимых решений задачи.
\end{rem}

Может оказаться, что наличие в определённой точке экстремума не
удаётся доказать, опираясь лишь на теорему \ref{th:kuhn-tucker} и
замечание \ref{rem:kt-cond}. В таком случае задачу исследуют с
применением условий высших порядков, которые приведены далее.

В следующих теоремах используются полные дифференциалы функции
Лагранжа $d^2\La$ и ограничений $dg_j$, определяемые следующим
образом:
\begin{align*}
  d^2\La &= \sum_{p=1}^n\sum_{r=1}^n{\dpardiff{\La}{x_p}{x_r}\,dx_pdx_r} \\
  dg_j &= \sum_{p=1}^n{\pardiff{g_j}{x_p}\,dx_p}
\end{align*}

\begin{thm}[Необходимое условие экстремума второго порядка]
  \label{th:if-extr-2}
  Пусть точка $\hat{x}$ — регулярный экстремум в задаче
  \eqref{eq:cond-optim-problem-form}, удовлетворяющий условиям теоремы
  \ref{th:kuhn-tucker} при соответствующем $\lambda$. Тогда
  \begin{align*}
    d^2\La(\hat{x}) &\geq 0 \qquad \text{для минимума} \\
    d^2\La(\hat{x}) &\leq 0 \qquad \text{для максимума}
  \end{align*}
  при $dx ≠ 0$ таких, что
  \begin{align*}
    dg_j(\hat{x}) &= 0 \qquad \forall j: \lambda_j ≠ 0\\
    dg_j(\hat{x}) &\leq 0 \qquad \forall j: \lambda_j=0
  \end{align*}
\end{thm}

\begin{thm}[Достаточное условие экстремума первого порядка]
  \label{th:then-extr-1}
  Пусть $x^*$ является регулярной условно-стационарной точкой задачи
  \eqref{eq:cond-optim-problem-form}, а число активных в $x^*$
  ограничений равно числу переменных $n$. Тогда для наличия в $x^*$
  регулярного экстремума достаточно выполнения одного из следующих
  наборов неравенств при $\forall j: g_j(\hat{x}) = 0$:
  \begin{align*}
    \lambda_j &> 0 \qquad \text{для минимума} \\
    \lambda_j &< 0 \qquad \text{для максимума}
  \end{align*}
\end{thm}

\begin{thm}[Достаточное условие экстремума второго порядка]
  \label{th:then-extr-2}
  Пусть $x^*$ является регулярной условно-стационарной точкой задачи
  \eqref{eq:cond-optim-problem-form}. Если при $dx ≠ 0$ таких, что
    \begin{align*}
    dg_j(x^*) &= 0 \qquad \forall j: \lambda_j ≠ 0\\
    dg_j(x^*) &\leq 0 \qquad \forall j: \lambda_j=0
  \end{align*}
  выполняется неравенство $d^2\La(x^*) ≠ 0$, то $x^*$ — точка
  регулярного экстремума, причём
  \begin{align*}
    d^2\La(x^*) &> 0 \qquad \text{для минимума} \\
    d^2\La(x^*) &< 0 \qquad \text{для максимума}
  \end{align*}
\end{thm}

\subsection{Практический пример}

Рассматривается следующая задача:
\begin{equation*}
  \begin{cases}
    f(x) = (x_1+4)^2 + (x_2-4)^2 \to \extr \\
    2x_1 - x_2 \leq 2 \\
    x_1 \geq 0 \\
    x_2 \geq 0
  \end{cases}
\end{equation*}

После приведения к каноническому виду она примет вид
\begin{equation}
  \label{eq:cond-optim-problem}
  \begin{cases}
    f(x) = (x_1+4)^2 + (x_2-4)^2 \to \extr \\
    g_1(x) = 2x_1 - x_2 - 2 \leq 0 \\
    g_2(x) = -x_1 \leq 0 \\
    g_3(x) = -x_2 \leq 0
  \end{cases}
\end{equation}

Заметим, что выполняется условие Слейтера \ref{th:slater}, так как в
качестве соответствующей точки $x_s$ можно взять, например, точку $(1,
1)$. Значит, в данной задаче достаточно рассмотреть лишь случай
классической функции Лагранжа с $\lambda_0=1$.

Составим функцию Лагранжа:
\begin{multline}
  \label{eq:lagrange}
  \La(x, \lambda) = (x_1+4)^2 + (x_2-4)^2 +\\
  + \lambda_1(2x_1-x_2-2)+\lambda_2(-x_1)+\lambda_3(-x_2)
\end{multline}

Запишем необходимые условия стационарности точки $x^*$ при
соответствующем векторе $\lambda$:
\begin{subequations}
  \renewcommand{\theequation}{\theparentequation\asbuk{equation}}
  \label{eq:kkt-conditions}
  \begin{equation}
    \label{eq:cond-stationary}
    \begin{cases}
      \pardiff{\La}{x_1} = 2(x^*_1+4)+2\lambda_1-\lambda_2=0\\
      \pardiff{\La}{x_2} = 2(x^*_2-4)-\lambda_1-\lambda_3=0
    \end{cases}
  \end{equation}
  \begin{equation}
    \label{eq:cond-sign}
    \sgn(\lambda_1) = \sgn(\lambda_2) = \sgn(\lambda_3)
  \end{equation}
  \begin{equation}
    \label{eq:cond-slackness}
    \begin{cases}
      \lambda_1\cdot g_1(x^*) = \lambda_1\cdot (2x^*_1-x^*_2-2) = 0\\
      \lambda_2\cdot g_2(x^*) = \lambda_2\cdot (-x^*_1) = 0\\
      \lambda_3\cdot g_3(x^*) = \lambda_3\cdot (-x^*_2) = 0
    \end{cases}
  \end{equation}
  \begin{equation}
    \begin{cases}
      \label{eq:cond-feasible}
      g_1(x^*) = 2x^*_1 - x^*_2 - 2 \leq 0 \\
      g_2(x^*) = -x^*_1 \leq 0 \\
      g_3(x^*) = -x^*_2 \leq 0
    \end{cases}
  \end{equation}
\end{subequations}

Рассмотрим $2³=8$ вариантов удовлетворения условий дополняющей
нежёсткости \eqref{eq:cond-slackness}.

\begin{enumerate}
\renewcommand{\labelenumi}{\Roman{enumi})}
\renewcommand{\labelenumiii}{\arabic{enumiii})}

\item $\lambda_1 = 0$
  
  В этом случае уравнения \eqref{eq:cond-stationary} принимают вид:
  \begin{equation}
    \label{eq:cond-stationary-l1=0}
    \begin{cases}
      2x^*_1+8-\lambda_2=0\\
      2x^*_2-8-\lambda_3=0
    \end{cases}
  \end{equation}
  \begin{enumerate}
  \item $\lambda_2 = 0$

    Из \eqref{eq:cond-stationary-l1=0} имеем $2x^*_1+8=0 \iff
    x^*_1=-4$, что не удовлетворяет условию $g_2(x^*) = -x^*_1 \leq 0$
    из \eqref{eq:cond-feasible}.
  \item $\lambda_2 ≠ 0$
    
    В этом случае из \eqref{eq:cond-slackness} следует, что
    $g_2(x^*)=0 \iff x^*_1 = 0$, откуда согласно
    \eqref{eq:cond-stationary-l1=0} получаем $\lambda_2=8$.
    \begin{enumerate}
    \item $\lambda_3 = 0$
      
      Из \eqref{eq:cond-stationary-l1=0} следует $x^*_2=4$. Получаем
      точку $A = (0, 4)$.
    \item $\lambda_3 ≠ 0$

      В данном случае согласно \eqref{eq:cond-slackness} находим
      $x^*_2=0$, поэтому из \eqref{eq:cond-stationary-l1=0} следует, что
      $\lambda_3=-8$. С учётом $\lambda_2=8$ заметим, что не
      выполняются условия \eqref{eq:cond-sign}.
    \end{enumerate}
  \end{enumerate}
\item $\lambda_1 ≠ 0$ 

  Согласно условию \eqref{eq:cond-slackness}, в
  данном случае
  \begin{equation}
    \label{eq:cond-slackness-l1n=0}
    2x^*_1-x^*_2-2=0
  \end{equation}
  \begin{enumerate}
  \item $\lambda_2 = 0$

    Из \eqref{eq:cond-stationary} получим
    \begin{equation}
      \label{eq:cond-stationary-l2=0}
      2x^*_1+8+2\lambda_1=0
    \end{equation}
    \begin{enumerate}
    \item $\lambda_3 = 0$

      Второе уравнение системы \eqref{eq:cond-stationary} даёт
      $2x^*_2-8-\lambda_1=0$. Сложим это уравнение с
      \eqref{eq:cond-stationary-l2=0} и рассмотрим его вместе с
      \eqref{eq:cond-slackness-l1n=0}, получив
      \begin{equation}
        \begin{cases}
          2x^*_1+4x^*_2-8=0\\
          2x^*_1-x^*_2-2=0
        \end{cases}
      \end{equation}
      
      Таким образом получим $4x^*_2-8=-x^*_2-2 \iff x^*_2=\frac{6}{5}$.

      Значение $x^*_1=\frac{8}{5}$ определяется из
      \eqref{eq:cond-slackness-l1n=0}. После этого из уравнения
      \eqref{eq:cond-stationary-l2=0} найдём значение $\lambda_1 =
      -\frac{28}{5}$. Итак, получена ещё одна точка $B =
      \left(\frac{8}{5}, \frac{6}{5}\right)$.
    \item $\lambda_3 ≠ 0$

      Согласно \eqref{eq:cond-slackness}, в данном случае $x^*_2=0$,
      поэтому из \eqref{eq:cond-slackness-l1n=0} следует $x^*_1=1$.
      Тогда из \eqref{eq:cond-stationary-l2=0} определим $\lambda_1 =
      -5$. Подставив найденные значения $x_2*$ и $\lambda_1$ в
      \eqref{eq:cond-stationary}, получим $\lambda_3 = -3$. Найдена
      очередная точка $C=(1, 0)$.
    \end{enumerate}
  \item $\lambda_2 ≠ 0$

    Из \eqref{eq:cond-slackness} получаем $x^*_1=0$, откуда с учётом
    \eqref{eq:cond-slackness-l1n=0} следует значение $x^*_2=-2$, не
    удовлетворяющее условию $g_3(x^*) = -x^*_2 \leq 0$ из
    \eqref{eq:cond-feasible}.
  \end{enumerate}
\end{enumerate}

Итак, найдены три точки, для которых выполнены необходимые условия
теоремы \ref{th:kuhn-tucker}. Тип возможного экстремума определяется
согласно знаку компонент $\lambda$.
\begin{itemize}
\item $A = (0, 4),\, \lambda=(0, 8, 0)$, минимум
\item $B = (\frac{8}{5}, \frac{6}{5}),\, \lambda=(-\frac{28}{5}, 0,
  0)$, максимум
\item $C = (1, 0),\, \lambda=(-5, 0, -3)$, максимум
\end{itemize}

\begin{figure}[!h]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      x=2 cm, y=1 cm,
      xmin=0, ymin=0, xmax=2.5
      xlabel=$x_1$, ylabel=$x_2$,
      enlargelimits=0.05]
      \input{condextr-contours.tkz.tex}
      \addplot[very thick, red!50!black] coordinates{(0,5) (0,0) (1,0) (2.5,3)};
      \label{plot:boundaries}
      
      \node[circle,fill=black,scale=0.5,label=right:$A$] at (axis cs:0,4) {};
      \node[circle,fill=black,scale=0.5,label=below right:$B$] at (axis cs:1.6,1.2) {};
      \node[circle,fill=black,scale=0.5,label=above:$C$] at (axis cs:1,0) {};
    \end{axis}
  \end{tikzpicture}
  \caption{Границы $g_j(x)=0$ (\ref{plot:boundaries}), линии уровня
    целевой функции $f(x)$ и условно-стационарные точки задачи
    \eqref{eq:cond-optim-problem}}
  \label{fig:cond-optim}
\end{figure}

Согласно замечанию \ref{rem:kt-cond}, поскольку и целевая функция
(поскольку её матрица Гессе $\left( \begin{smallmatrix}2 & 0 \\ 0 &
    2\end{smallmatrix} \right)$ положительно определена), и
рассматриваемое множество (см. рис. \ref{fig:cond-optim}) обладают
свойством выпуклости, для точки $A$ условия Куна—Таккера являются
\emph{достаточными}, и она является точкой \emph{локального минимума}.
В то же время, для $B$ и $C$ условия Куна—Таккера не являются
достаточными.

Проверим для точек $B$ и $C$ условия высших порядков.

Второй дифференциал $d^2\La$ во всех точках одинаковый и имеет вид
\begin{equation}
  \label{eq:la-diff}
  d^2\La = 2dx_1^2 + 2 dx_2^2
\end{equation}

\begin{itemize}
\item $B = (\frac{8}{5}, \frac{6}{5})$

  В данной точке активно лишь ограничение $g_1$, так что
  воспользоваться достаточным условием первого порядка нельзя.
  Проверим необходимое условие второго порядка. Приравнивая к нулю
  дифференциал $dg_1$ активного ограничения и выбирая дифференциалы
  неактивных ограничений неположительными, получим
  \begin{align*}
    dg_1 &= 2dx_1-dx_2 = 0 \iff dx_2 = 2dx_1 \\
    dg_2 &= -dx_1 \leq 0 \\
    dg_3 &= -dx_2 \leq 0
  \end{align*}
  и с учётом этого рассмотрим \eqref{eq:la-diff} при условии $dx≠0$:
  \begin{equation*}
    d^2\La = 2dx_1^2 + 8dx_1^2 = 10dx_1^2
  \end{equation*}
  
  Полученная форма очевидно больше нуля. Поскольку в $B$ значения
  $\lambda < 0$, необходимое условие второго порядка теоремы
  \ref{th:if-extr-2} не выполняется, а $B$ \emph{не} является точкой
  экстремума.

\item $C = (1, 0)$

  Активными являются ограничения $g_1$ и $g_3$, их количество равно
  числу переменных $n=2$, поэтому согласно теореме
  \ref{th:then-extr-1} выполняется достаточное условие экстремума
  первого порядка, и $C$ есть точка \emph{локального максимума}.

  Действительно, из иллюстрации \ref{fig:cond-optim} видно, что любое
  сколь угодно малое допустимое перемещение из точки $C$ приводит к
  переходу с линии уровня $f(C)=41$ на линии меньших уровней.
  
  \begin{figure}[!h]
    \centering
    \begin{tikzpicture}
      \begin{axis}[
        xlabel=$x_1$, ylabel=$x_2$,
        enlargelimits=0.05]
        \input{condextr_zoom-contours.tkz.tex}
        \addplot[very thick, red!50!black] coordinates{(0.85,0) (1,0) (1.15,0.3)};
        \node[circle,fill=black,scale=0.5,label=right:$C$] at (axis cs:1,0) {};
      \end{axis}
    \end{tikzpicture}
    \caption{Линии уровня функции $f(x) = (x_1+4)^2+(x_2-4)^2$ вблизи
      точки $C$}
    \label{fig:cond-optim}
  \end{figure}
\end{itemize}
  
\clearpage
\section{Многопараметрическая оптимизация\\
  с чебышёвскими функциями релаксации}
\label{sec:relch}

В данном разделе представлена общая схема градиентных методов,
рассмотрено понятие функции релаксации и описан метод оптимизации с
чебышёвскими функциями релаксации, предложенный в
\cite{chernorutsky04}.

\subsection{Теоретические сведения}

\subsubsection{Общая схема градиентных методов}

Рассмотрим задачу безусловной минимизации:
\begin{equation*}
  f(x) \to \min,\quad x \in \set{R}^n,\, J \in C^2(\set{R}^n)
\end{equation*}

\begin{dfn}
  \neword{Градиентными} называются итерационные методы оптимизации со
  следующей рабочей формулой, которая определяет способ перехода к
  новому приближению $x^{k+1}$ на очередном шаге итерации:
  \begin{equation}
    \label{eq:grad-methods}
    x^{k+1} = x^k - H_k\left(G_k, h_k\right) g_k
  \end{equation}
  здесь $H_k$ — некоторая функция от матрицы Гессе $G_k = G(x^k) =
  f''(x^k)$ и параметра $h_k$, а $g_k = g(x^k) = f'(x^k)$ — градиент
  функции в точке $x_k$.
\end{dfn}

При постоянных значениях параметров на различных шагах итерации
соответствующие индексы $k$ в формуле \eqref{eq:grad-methods}
опускаются.

Предполагается, что в некоторой $\epsilon_k$-окрестности $\{x \in
\set{R}^n | \norm{x-x^k} < \epsilon_k\}$ точки $x^k$ функция $f(x)$
приближается гиперболоидом:
\begin{equation}
  \label{eq:sqr-approx}
  f(x) \approx \frac{1}{2}\scalmult{G_k x, x} - \scalmult{a_k,x} + b_k \approx \frac{1}{2}\scalmult{G_k x, x}
\end{equation}

Ставится задача построения таких матричных функций $H_k$, при которых
выполняется \neword{условие релаксации} процесса
\begin{equation}
  \label{eq:relax-cond}
  f(x^{k+1}) < f(x^k)
\end{equation}
При этом требуется, чтобы величина нормы $\norm{x^{k+1}-x^{k}}$ была
ограничена сверху лишь параметром $\epsilon_k$, который характеризует
область справедливости локальной квадратичной модели
\eqref{eq:sqr-approx}.

\subsubsection{Функция релаксации}

\begin{dfn}
  \neword{Функцией релаксации} называется скалярная функция
  \begin{equation}
    \label{eq:relax-fun}
    R_h(\lambda) = 1 - H(\lambda, h)\lambda,\quad \lambda,h \in \set{R}
  \end{equation}
  где $H(\lambda, h)$ — скалярный аналог матричной функции $H(G, h)$
  из формулы \eqref{eq:grad-methods}.
\end{dfn}
В дальнейшем индекс $h$ у функции релаксации $R_h(\lambda)$ иногда
будем опускать.

\begin{dfn}
  \neword{Множителями релаксации} для точки $x^k$ называются значения
  функции релаксации на спектре матрицы Гессе:
  \begin{equation}
    \label{eq:relax-fac}
    R_h(\lambda_i),\, \lambda_i \in \Sp{G_k}
  \end{equation}
\end{dfn}

Благодаря следующей теореме, функция релаксации используется для
анализа различных градиентных методов.

\begin{thm}
  \label{thm:relax-thm}
  При любых $x^k$ для выполнения условия релаксации
  \eqref{eq:relax-cond} необходимо и достаточно, чтобы
  \begin{equation}
    \label{eq:relax-thm}
    \begin{aligned}
      & \abs{R(\lambda_i)} \geq 1 & \lambda_i& < 0 \\
      & \abs{R(\lambda_i)} \leq 1 & \lambda_i& > 0\\
      &&i& = \overline{1, n}
    \end{aligned}
  \end{equation}
\end{thm}

\input{relax-thm.tkz}

Скорость релаксации может быть оценена с использованием следующего
соотношения:
\begin{equation}
  \label{eq:relax-speed}
  2\abs{f(x^{k+1})-f(x^k)}=\sum_{\lambda_i^+>0} \left\{\xi_{i,k}^2
    \lambda_i^+ [1-R^2(\lambda_i^+)]\right\} + \sum_{\lambda_i^-<0} \left\{\xi_{i,k}^2
    \abs{\lambda_i^-} [R^2(\lambda_i^-)-1]\right\}
\end{equation}
здесь $\lambda_i^+$ и $\lambda_i^-$ — положительные и отрицательные
собственные значения матрицы $G_k$. Коэффициенты $\xi_{i,k}$
происходят из разложения $x^k=\sum_i{\xi_{i,k}u^i}$ по собственным
векторам $u^i$ матрицы Гессе.

Таким образом, эффективными оказываются методы, функция релаксации
которых в положительной области значений $\lambda$ как можно
\emph{меньше} уклоняется от нуля, а при отрицательных $\lambda$ —
становится как можно большей по модулю.

\subsubsection{Метод ПГС}

В качестве примера рассмотрим метод \neword{простого градиентного
  спуска} (ПГС), рабочая формула которого имеет вид:
\begin{equation*}
  x^{k+1}=x^k-hg_k,\, h\in\set{R}
\end{equation*}
Рассмотрим функцию релаксации метода ПГС:
\begin{equation*}
  R(\lambda) = 1 - \lambda h
\end{equation*}

\input{gd-relax.tkz}

Из её графика на рисунке \ref{fig:relax-thm} видно, что метод применим
лишь в том случае, когда собственные значения матрицы Гессе
оптимизируемой функции не превосходят некоторого критического значения
$M*=\frac{2}{h}$. Кроме того, в области значений $\lambda$, близких к
нулю или $M*$, согласно \eqref{eq:relax-speed} скорость релаксации
сильно снижается.

\subsection{Примеры работы}
Одним из классических тестов для различных алгоритмов оптимизации
является тест Розенброка, заключающийся в минимизации следующей
функции:
\begin{equation}
  \label{eq:rosenbrock}
  f(x, y) = 100(y - x²)² + (1 - x)²
\end{equation}
Функция Розенброка имеет глубокую впадину вдоль кривой $y=x^2$, что
обуславливает плохую сходимость простых методов.

\pgfplotsset{every axis/.append style={xlabel=$x$, ylabel=$y$,grid}}
\pgfplotsset{every axis grid/.append style={densely dashed}}
\begin{figure}[!thb]
  \centering
  \begin{tikzpicture}[scale=1]
    \begin{axis}
      \input{rosenbrock-contours.tkz.tex}
      \input{rosenbrock_relch_-1.2,1_40_14-trace.tkz.tex}
    \end{axis}
  \end{tikzpicture}
  \caption{Работа алгоритма с функцией Розенброка
    \eqref{eq:rosenbrock}}
\end{figure}

Теперь рассмотрим функцию
\begin{equation}
  \label{eq:exptest}
  f(x, y) = \sum\limits_{a=\overline{0.1, 1.0}}\left [
    e^{-xa}-e^{-ya}-(e^{-a}-e^{-10a})\right ]^2
\end{equation}
Здесь суммирование происходит по значениям $a = 0.1, 0.2, \dotsc, 1$.

\begin{figure}[thb]
  \centering
  \begin{tikzpicture}[scale=1]
    \begin{axis}
      \input{exptest-contours.tkz.tex}
      \input{exptest_relch_-0.5,1.5_20-trace.tkz.tex}
      \input{exptest_relch_5,5_20-trace.tkz.tex}
      \input{exptest_relch_3,-1_20-trace.tkz.tex}
    \end{axis}
  \end{tikzpicture}
  \caption{Линии уровня и трассировка процесса минимизации функции
    \eqref{eq:exptest}}
\end{figure}

В следующем примере рассмотрим функцию Химмельблау, которая задаётся
следующим образом:
\begin{equation}
  \label{eq:himmelblau}
  f(x, y) = (x² + y - 11)² + (x + y² - 7)²
\end{equation}
Известно, что локальные минимумы функции \eqref{eq:himmelblau}
расположены в точках $A=(-3.78, -3.28)$, $B=(-2.80, 3.13)$, $C=(3.58,
-1.85)$, $D=(3.00, 2.00)$, и в каждой из них достигается значение $0$.
Этим обусловлена следующая особенность работы алгоритма — результат
значительно зависит от выбора начального приближения, что
продемонстрировано на \ref{fig:himmelblau}.

\begin{figure}[thb]
  \label{fig:himmelblau}
  \centering
  \begin{tikzpicture}[scale=1]
    \begin{axis}
      \input{himmelblau-contours.tkz.tex}
      \input{himmelblau_relch_0.1,0.1_10-trace.tkz.tex}
      \input{himmelblau_relch_0.5,-0.5_10-trace.tkz.tex}
      \input{himmelblau_relch_-0.5,0.5_10-trace.tkz.tex}
      \input{himmelblau_relch_-0.5,-0.5_10-trace.tkz.tex}
    \end{axis}      
  \end{tikzpicture}
  \caption{Результаты работы алгоритма с функцией Химмельблау в
    зависимости от выбора начальной точки}
\end{figure}

\clearpage
\appendix
\bibliographystyle{gost71s}
\bibliography{paper}

\end{document}
