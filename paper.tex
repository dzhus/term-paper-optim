\documentclass{article}
\usepackage[utf8x]{inputenc}
\usepackage[english,russian]{babel}
\usepackage{amsmath,amsthm,amssymb}

% Rich title
\usepackage{titling}

% Matrix operations
\usepackage{gauss}

\usepackage{pgfplots}
\usetikzlibrary{patterns}

% Russian traditions
\renewcommand{\epsilon}{\varepsilon}
\renewcommand{\phi}{\varphi}
\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}
\newcommand{\intl}{\int\limits}
\usepackage{misccorr}

% Bib in TOC
\usepackage[numbib,nottoc]{tocbibind}

\usepackage{tocbibind}

% Custom commands
\providecommand{\program}[1]{{\tt #1}}
\providecommand{\La}{\mathcal{L}}
\providecommand{\neword}{\emph}
\providecommand{\pardiff}[2]{\frac{\partial{#1}}{\partial{#2}}}
\providecommand{\abs}[1]{\left \lvert{#1}\right \rvert}
\providecommand{\norm}[1]{\left \lVert{#1}\right \rVert}
\providecommand{\set}[1]{\mathbb{#1}}
\newcommand{\scalmult}[1]{{\left \langle #1 \right \rangle}}
\DeclareMathOperator{\extr}{extr}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\Sp}{Sp}

\newtheorem{thm}{Теорема}
\newtheorem{rem}{Замечание}

\numberwithin{equation}{section}

\usepackage[pdftex,unicode]{hyperref}

\begin{document}
\author{Дмитрий Джус}
\title{Курсовая работа по теме \\
  \Huge{«Методы оптимизации»}}
\pretitle{\begin{center}\LARGE}
\posttitle{\par\end{center}\vskip 3pc}
\date{}
\maketitle
\thispagestyle{empty}

\clearpage
\tableofcontents

\clearpage
\section{Введение}

Настоящая курсовая работа посвящена методам оптимизации и
экстремальным задачам.

В первой части рассмотрено решение задачи условной минимизации
линейной функции с помощью симплекс-метода.

Кроме того, дано описание градиентного метода многопараметрической
оптимизации с чебышёвским функциями релаксации, предложена реализация
метода на алгоритмическом языке и представлены результаты работы на
тестовых функциях.

\section{Симплекс-метод}

Симплекс-метод позволяет решать задачи линейного программирования,
заключающиеся в минимизации целевого линейного функционала при
заданных линейных ограничениях. Основной идеей симплекс-метода
является перебор вершин выпуклого многогранника в многомерном
пространстве. Описание и общую схему метода можно найти в
\cite{taha05}.

К решению предлагается следующая задача:
\begin{equation}
  \label{eq:lp-initial}
  \begin{cases}
    f(x) = -3x_1+2x_2-2x_3+2x_4-x_5 \to \min \\
    -x_1+x_2-x_3=1 \\
    -x_2+x_3+x_4=1 \\
    \phantom{-}x_2+x_3+x_5=2 \\
    \phantom{-}x_i \geq 0,\, i = \overline{1, 5}
  \end{cases}
\end{equation}

Для того, чтобы матрица коэффициентов ограничений содержала в себе
единичную, воспользуемся методом искусственных переменных, добавив в
первое ограничение переменную $x_6$ так, что оно примет вид
\begin{equation*}
  -x_1+x_2-x_3+x_6=1 \\
\end{equation*}

Заменим минимизируемую функцию на $\tilde{f}(x) = x_6$ и выразим её
через свободные переменные $x_1, x_2, x_3$, получив
\mbox{$\tilde{f}(x) = x_1-x_2+x_3+1$}. Перейдём к рассмотрению
вспомогательной задачи
\begin{equation}
  \begin{cases}
    \tilde{f}(x) = x_1-x_2+x_3+1 \to \min \\
    -x_1+x_2-x_3+x_6=1 \\
    -x_2+x_3+x_4=1 \\
    \phantom{-}x_2+x_3+x_5=2 \\
    \phantom{-}x_i \geq 0,\, i = \overline{1, 6}
  \end{cases}
\end{equation}

Решим её симплекс-методом. Составим расширенную матрицу из
коэффициентов ограничений и целевой функции:
\begin{equation}
  \begin{gmatrix}[b]
    \dagger & x_1 & x_2 & x_3 & x_4 & x_5 & x_6 & \diamond\\
    x_6 & -1 &  1 & -1 & 0 & 0 & 1 & \mathbf{1} \\
    x_4 &  0 & -1 &  1 & 1 & 0 & 0 & 1 \\
    x_5 &  0 &  1 &  1 & 0 & 1 & 0 & 2 \\
    \tilde{f}    & -1 &  \mathbf{1} & -1 & 0 & 0 & 0 & 1
  \end{gmatrix}
\end{equation}

Базисными переменными являются $x_4, x_5, x_6$. Для дальнейшего
решения необходимо исключить искусственную переменную $x_6$ из состава
базисных. 

Выберем в качестве ведущего столбец с максимальным элементом в
последней строке — это столбец переменной $x_2$, поскольку $\max\{1,
-1, 0\} = 1$. С помощью симплексного отношения выберем первую строку в
качестве ведущей, поскольку $\min\left\{\frac{1}{1},
  \frac{2}{1}\right\} = 1$.

Учитывая выбранный ведущий элемент, переведём переменную $x_2$ в
состав базисных взамен $x_6$. Сложим со второй строкой первую, а с
третьей и четвёртой — первую, умноженную на $-1$:
\begin{equation*}
  \begin{gmatrix}[b]
    \dagger & x_1 & x_2 & x_3 & x_4 & x_5 & x_6 & \diamond\\
    x_6 & -1 &  \mathbf{1} & -1 & 0 & 0 & 1 & 1 \\
    x_4 &  0 & -1 &  1 & 1 & 0 & 0 & 1 \\
    x_5 &  0 &  1 &  1 & 0 & 1 & 0 & 2 \\
    \tilde{f} & -1 &  1 & -1 & 0 & 0 & 0 & 1
    \rowops
    \add{1}{2}
    \add[-1]{1}{3}
    \add[-1]{1}{4}
  \end{gmatrix}
\end{equation*}

Получим матрицу с базисными переменными $x_2, x_4, x_5$:
\begin{equation}
  \label{eq:lp-artif}
  \begin{bmatrix}
    \dagger & x_1 & x_2 & x_3 & x_4 & x_5 & x_6 & \diamond\\
    x_2 & -1 &  1 & -1 & 0 & 0 & 1 &  1\\
    x_4 & -1 &  0 &  0 & 1 & 0 & 0 &  2\\
    x_5 &  1 &  0 &  2 & 0 & 1 & 0 &  1\\
    \tilde{f} & 0 &  0 & 0 & 0 & 0 & -1 & 0
  \end{bmatrix}
\end{equation}

При этом значение функции $\tilde{f}(x)$ равно нулю, что свидетельствует
о том, что найденное решение вспомогательной задачи является
допустимым для исходной.

Отбросим искусственную переменную $x_6$ и вернёмся к начальной задаче
\eqref{eq:lp-initial}. Исключим из исходной целевой функции $f(x)$
базисные переменные $x_2, x_4$ и $x_5$, используя ограничения из
матрицы \eqref{eq:lp-artif}:
\begin{equation*}
  \begin{cases}
    -x_1+x_2-x_3=1\\
    -x_1+x_4=2\\
    \phantom{-}x_1+2x_3+x_5 = 1
  \end{cases}
\end{equation*}

Тогда $f(x)$ принимает вид
\begin{equation*}
  f(x) = 2x_1+2x_3+5
\end{equation*}

Используем эту целевую функцию в матрице ограничений \eqref{eq:lp-artif}:
\begin{equation*}
  \begin{bmatrix}
    \dagger & x_1 & x_2 & x_3 & x_4 & x_5 & \diamond\\
    x_2 & -1 & 1 & -1 & 0 & 0 & 1\\
    x_4 & -1 & 0 &  0 & 1 & 0 & 2\\
    x_5 &  1 & 0 &  2 & 0 & 1 & 1\\
    \tilde{f} & -2 & 0 & -2 & 0 & 0 & 5
  \end{bmatrix}
\end{equation*}

В нижнем ряду под столбцами переменных все коэффициенты отрицательны,
что свидетельствует о том, что найдено оптимальное решение:

\begin{equation}
  \begin{cases}
    x_1 = 0\\
    x_2 = 1\\
    x_3 = 0\\
    x_4 = 2\\
    x_5 = 1
  \end{cases}
\end{equation}

\clearpage
\section{Условная оптимизация}

\subsection{Теоретические сведения}

Сформулируем без доказательства ряд условий, которые используются при
решении задач условной оптимизации нелинейных функционалов. 


\begin{thm}[Куна—Таккера]
  \label{th:kuhn-tucker}

  Пусть рассматривается следующая задача:
  \begin{equation*}
    \begin{cases}
      f(x) \to \extr \\
      g_j(x) \leq 0,\, j=\overline{1,m}
    \end{cases}
  \end{equation*}
  Здесь $x$ — вектор $x_1, \dotsc x_n$.

  Составляется \emph{функция Лагранжа}:
  \begin{equation*}
    \La(x, \lambda_0, \lambda) = \lambda_0 f(x) + \sum_{j=1}^m {\lambda_j g_j(x)}
  \end{equation*}
  Здесь $\lambda$ — вектор $\lambda_1, \dotsc \lambda_m$.
    
  Пусть точка $x^*$ — точка экстремума. Тогда выполнены следующие условия:
  \begin{enumerate}
    \renewcommand{\labelenumi}{\emph{\asbuk{enumi})}}
  \item $\pardiff{\La}{x_i}=0,\, i=\overline{1,n}$ (условие
    стационарности функции Лагранжа)
  \item $\lambda_j \geq 0,\, j=\overline{1,m}$, если $x^*$ — точка
    минимума, и $\lambda_j \leq 0$, если это точка максимума.
  \item $\lambda_j \cdot g_j(x^*) = 0,\, j=\overline{1,m}$ (условие
    дополняющей нежёсткости)
  \item $g_j(x^*) \leq 0,\, j=\overline{1,m}$
  \item $\lambda_0^2 + \norm{\lambda}^2 > 0$ (условие нетривиальности решения)
  \end{enumerate}
\end{thm}

\begin{thm}[Условие Слейтера]
  Для $\lambda_0 \neq 0$ достаточно существования такой точки
  $\hat{x}$, в которой все неравенства ограничений выполняются строго:
  $g_j(\hat{x})<0,\, j=\overline{1,m}$. В таком случае без ограничения
  общности полагают $\lambda_0=1$.
\end{thm}

\begin{rem}
  Для точек минимума необходимые условия Куна—Таккера становятся
  достаточными в случае, когда целевая функция $f(x)$ и ограниченное
  неравенствами $g_j(x)$ множество \emph{выпуклы}.

  В случае точек максимума достаточность достигается при
  \emph{вогнутости} функции $f(x)$ и \emph{выпуклости} множества
  допустимых решений задачи.
\end{rem}

\subsection{Практический пример}

Рассматривается следующая задача:
\begin{equation*}
  \begin{cases}
    f(x) = (x_1+4)^2 + (x_2-4)^2 \to \extr \\
    2x_1 - x_2 \leq 2 \\
    x_1 \geq 0 \\
    x_2 \geq 0
  \end{cases}
\end{equation*}

После приведения к каноническому виду она примет вид
\begin{equation}
  \begin{cases}
    f(x) = (x_1+4)^2 + (x_2-4)^2 \to \extr \\
    g_1(x) = 2x_1 - x_2 - 2 \leq 0 \\
    g_2(x) = -x_1 \leq 0 \\
    g_3(x) = -x_2 \leq 0
  \end{cases}
\end{equation}

Составим функцию Лагранжа:
\begin{multline}
  \label{eq:lagrange}
  \La(x, \lambda) = (x_1+4)^2 + (x_2-4)^2 +\\
  + \lambda_1(2x_1-x_2-2)+\lambda_2(-x_1)+\lambda_3(-x_2)
\end{multline}

Запишем необходимые условия стационарности точки $x^*$ при
соответствующем векторе $\lambda$:
\begin{subequations}
  \renewcommand{\theequation}{\theparentequation\asbuk{equation}}
  \label{eq:kkt-conditions}
  \begin{equation}
    \label{eq:cond-stationary}
    \begin{cases}
      \pardiff{\La}{x_1} = 2(x^*_1+4)+2\lambda_1-\lambda_2=0\\
      \pardiff{\La}{x_2} = 2(x^*_2-4)-\lambda_1-\lambda_3=0
    \end{cases}
  \end{equation}
  \begin{equation}
    \label{eq:cond-sign}
    \sgn(\lambda_1) = \sgn(\lambda_2) = \sgn(\lambda_3)
  \end{equation}
  \begin{equation}
    \label{eq:cond-slackness}
    \begin{cases}
      \lambda_1\cdot g_1(x^*) = \lambda_1\cdot (2x^*_1-x^*_2-2) = 0\\
      \lambda_2\cdot g_2(x^*) = \lambda_2\cdot (-x^*_1) = 0\\
      \lambda_3\cdot g_3(x^*) = \lambda_3\cdot (-x^*_2) = 0
    \end{cases}
  \end{equation}
  \begin{equation}
    \begin{cases}
      \label{eq:cond-feasible}
      g_1(x^*) = 2x^*_1 - x^*_2 - 2 \leq 0 \\
      g_2(x^*) = -x^*_1 \leq 0 \\
      g_3(x^*) = -x^*_2 \leq 0
    \end{cases}
  \end{equation}
\end{subequations}

Рассмотрим $2³=8$ вариантов удовлетворения условий дополняющей
нежёсткости \eqref{eq:cond-slackness}.

\begin{enumerate}
\renewcommand{\labelenumi}{\Roman{enumi})}
\renewcommand{\labelenumiii}{\arabic{enumiii})}

\item $\lambda_1 = 0$
  
  В этом случае уравнения \eqref{eq:cond-stationary} принимают вид:
  \begin{equation}
    \label{eq:cond-stationary-l1=0}
    \begin{cases}
      2x^*_1+8-\lambda_2=0\\
      2x^*_2-8-\lambda_3=0
    \end{cases}
  \end{equation}
  \begin{enumerate}
  \item $\lambda_2 = 0$

    Из \eqref{eq:cond-stationary-l1=0} имеем $2x^*_1+8=0 \iff
    x^*_1=-4$, что не удовлетворяет условию $g_2(x^*) = -x^*_1 \leq 0$
    из \eqref{eq:cond-feasible}.
  \item $\lambda_2 ≠ 0$
    
    В этом случае из \eqref{eq:cond-slackness} следует, что
    $g_2(x^*)=0 \iff x^*_1 = 0$, откуда согласно
    \eqref{eq:cond-stationary-l1=0} получаем $\lambda_2=8$.
    \begin{enumerate}
    \item $\lambda_3 = 0$
      
      Из \eqref{eq:cond-stationary-l1=0} следует $x^*_2=4$. Получаем
      точку $A = (0, 4)$.
    \item $\lambda_3 ≠ 0$

      В данном случае согласно \eqref{eq:cond-slackness} находим
      $x^*_2=0$, поэтому из \eqref{eq:cond-stationary-l1=0} следует, что
      $\lambda_3=-8$. С учётом $\lambda_2=8$ заметим, что не
      выполняются условия \eqref{eq:cond-sign}.
    \end{enumerate}
  \end{enumerate}
\item $\lambda_1 ≠ 0$ 

  Согласно условию \eqref{eq:cond-slackness}, в
  данном случае
  \begin{equation}
    \label{eq:cond-slackness-l1n=0}
    2x^*_1-x^*_2-2=0
  \end{equation}
  \begin{enumerate}
  \item $\lambda_2 = 0$

    Из \eqref{eq:cond-stationary} получим
    \begin{equation}
      \label{eq:cond-stationary-l2=0}
      2x^*_1+8+2\lambda_1=0
    \end{equation}
    \begin{enumerate}
    \item $\lambda_3 = 0$

      Второе уравнение системы \eqref{eq:cond-stationary} даёт
      $2x^*_2-8-\lambda_1=0$. Сложим это уравнение с
      \eqref{eq:cond-stationary-l2=0} и рассмотрим его вместе с
      \eqref{eq:cond-slackness-l1n=0}, получив
      \begin{equation}
        \begin{cases}
          2x^*_1+4x^*_2-8=0\\
          2x^*_1-x^*_2-2=0
        \end{cases}
      \end{equation}
      
      Таким образом получим $4x^*_2-8=-x^*_2-2 \iff x^*_2=\frac{6}{5}$.

      Значение $x^*_1=\frac{8}{5}$ определяется из
      \eqref{eq:cond-slackness-l1n=0}. После этого из уравнения
      \eqref{eq:cond-stationary-l2=0} найдём значение $\lambda_1 =
      -\frac{28}{5}$. Итак, получена ещё одна точка $B =
      \left(\frac{8}{5}, \frac{6}{5}\right)$.
    \item $\lambda_3 ≠ 0$

      Согласно \eqref{eq:cond-slackness}, в данном случае $x^*_2=0$,
      поэтому из \eqref{eq:cond-slackness-l1n=0} следует $x^*_1=1$.
      Тогда из \eqref{eq:cond-stationary-l2=0} определим $\lambda_1 =
      -5$. Подставив найденные значения $x_2*$ и $\lambda_1$ в
      \eqref{eq:cond-stationary}, получим $\lambda_3 = -3$. Найдена
      очередная точка $C=(1, 0)$.
    \end{enumerate}
  \item $\lambda_2 ≠ 0$

    Из \eqref{eq:cond-slackness} получаем $x^*_1=0$, откуда с учётом
    \eqref{eq:cond-slackness-l1n=0} следует значение $x^*_2=-2$, не
    удовлетворяющее условию $g_3(x^*) = -x^*_2 \leq 0$ из
    \eqref{eq:cond-feasible}.
  \end{enumerate}
\end{enumerate}

Итак, найдены три точки, для которых выполнены необходимые условия
стационарности. Тип возможного экстремума определяется согласно знаку
компонент $\lambda$.
\begin{itemize}
\item $A = (0, 4),\, \lambda=(0, 8, 0)$, минимум
\item $B = (\frac{8}{5}, \frac{6}{5}),\, \lambda=(-\frac{28}{5}, 0,
  0)$, максимум
\item $C = (1, 0),\, \lambda=(-5, 0, -3)$, максимум
\end{itemize}

И целевая функция (поскольку её матрица Гессе
$\left( \begin{smallmatrix}2 & 0 \\ 0 & 2\end{smallmatrix} \right)$
положительно определена), и рассматриваемое множество (см. рис.
\ref{fig:cond-extr}) обладают свойством \emph{выпуклости}, поэтому для
точки $A$ условия Куна—Таккера являются \emph{достаточными}, и она
является точкой локального минимума.

Для $B$ и $C$ условия Куна—Таккера были бы достаточными в случае
\emph{вогнутости} целевой функции и выпуклости ограничивающего
множества, что в рассматриваемом случае неверно.

Проверим для точек $B$ и $C$ достаточные условия первого порядка.

Второй дифференциал $d^2\La$ во всех точках одинаковый и имеет вид
\begin{equation}
  \label{eq:la-diff}
  d^2\La = 2dx_1^2 + 2 dx_2^2
\end{equation}

\begin{itemize}
\item $B = (\frac{8}{5}, \frac{6}{5})$

  В данной точке активно лишь ограничение $g_1$, так что
  воспользоваться достаточным условием первого порядка нельзя.
  Проверим необходимое условие второго порядка. Приравнивая к нулю
  дифференциал $dg_1$ активного ограничения и выбирая дифференциалы
  неактивных ограничений неположительными, получим
  \begin{align*}
    dg_1 &= 2dx_1-dx_2 = 0 \iff dx_2 = 2dx_1 \\
    dg_2 &= -dx_1 \leq 0 \\
    dg_3 &= -dx_2 \leq 0
  \end{align*}
  и с учётом этого рассмотрим \eqref{eq:la-diff} при условии $dx≠0$:
  \begin{equation*}
    d^2\La = 2dx_1^2 + 8dx_1^2 = 10dx_1^2
  \end{equation*}
  
  Полученная форма очевидно больше нуля. Поскольку в $B$ значения
  $\lambda < 0$, необходимое условие второго порядка не выполняется, а
  $B$ \emph{не} является точкой экстремума.

\item $C = (1, 0)$

  Активными являются не все ограничения, поэтому используем условия
  второго порядка. Положим дифференциалы активных ограничений $dg_1$ и
  $dg_3$ нулевыми, а дифференциал неактивного ограничения $dg_2$ —
  неположительным:
  \begin{align*}
    \left.
      \begin{aligned}
        dg_1 &= 2dx_1-dx_2 = 0 \\
        dg_3 &= - dx_2 = 0 \\
      \end{aligned}
    \right \} \implies dx_1 = dx_2 &= 0 \\
    dg_2 = - dx_1 &\leq 0
  \end{align*}
  
  Тогда \eqref{eq:la-diff} не может принять отличное от нуля значение:
  \begin{equation}
    d^2\La = 0
  \end{equation}
  так что выполнено необходимое, но не достаточное условие второго
  порядка.
\end{itemize}
  
\clearpage
\section{Многопараметрическая оптимизация\\
  с чебышёвскими функциями релаксации}

В данном разделе представлена общая схема градиентных методов,
рассмотрено понятие функции релаксации и описан метод оптимизации с
чебышёвскими функциями релаксации, предложенный в
\cite{chernorutsky04}.

\subsection{Теоретические сведения}

\subsubsection{Общая схема градиентных методов}

Рассмотрим задачу безусловной минимизации:
\begin{equation*}
  f(x) \to \min,\quad x \in \set{R}^n,\, J \in C^2(\set{R}^n)
\end{equation*}

\neword{Градиентными} называются итерационные методы оптимизации со
следующей рабочей формулой, которая определяет способ перехода к
новому приближению $x^{k+1}$ на очередном шаге итерации:
\begin{equation}
  \label{eq:grad-methods}
  x^{k+1} = x^k - H_k\left(G_k, h_k\right) g_k
\end{equation}
здесь $H_k$ — некоторая функция от матрицы Гессе $G_k = G(x^k) =
f''(x^k)$ и параметра $h_k$, а $g_k = g(x^k) = f'(x^k)$ — градиент
функции в точке $x_k$. При постоянных значениях параметров на
различных шагах итерации соответствующие индексы $k$ опускаются.

Предполагается, что в некоторой $\epsilon_k$-окрестности $\{x \in
\set{R}^n | \norm{x-x^k} < \epsilon_k\}$ точки $x^k$ функция $f(x)$
приближается гиперболоидом:
\begin{equation}
  \label{eq:sqr-approx}
  f(x) \approx \frac{1}{2}\scalmult{G_k x, x} - \scalmult{a_k,x} + b_k \approx \frac{1}{2}\scalmult{G_k x, x}
\end{equation}

Ставится задача построения таких матричных функций $H_k$, при которых
выполняется условие релаксации процесса
\begin{equation}
  \label{eq:relax-cond}
  f(x^{k+1}) < f(x^k)
\end{equation}
При этом требуется, чтобы величина нормы $\norm{x^{k+1}-x^{k}}$ была
ограничена сверху лишь параметром $\epsilon_k$, который характеризует
область справедливости локальной квадратичной модели
\eqref{eq:sqr-approx}.

\neword{Функцией релаксации} называется скалярная функция
\begin{equation}
  \label{eq:relax-fun}
  R_h(\lambda) = 1 - H(\lambda, h)\lambda,\quad \lambda,h \in \set{R}
\end{equation}
где $H(\lambda, h)$ — скалярный аналог матричной функции $H(G, h)$ из
формулы \eqref{eq:grad-methods}.

В дальнейшем индекс $h$ у функции релаксации $R_h(\lambda)$ иногда
будем опускать.

\neword{Множителями релаксации} для точки $x^k$ называются значения
функции релаксации на спектре матрицы Гессе:
\begin{equation}
  \label{eq:relax-fac}
  R_h(\lambda_i),\, \lambda_i \in \Sp{G_k}
\end{equation}

Благодаря следующей теореме, функция релаксации используется для
анализа различных градиентных методов.

\begin{thm}
  \label{thm:relax-thm}
  При любых $x^k$ для выполнения условия релаксации
  \eqref{eq:relax-cond} необходимо и достаточно, чтобы
  \begin{equation}
    \label{eq:relax-thm}
    \begin{aligned}
      & \abs{R(\lambda_i)} \geq 1 & \lambda_i& < 0 \\
      & \abs{R(\lambda_i)} \leq 1 & \lambda_i& > 0\\
      &&i& = \overline{1, n}
    \end{aligned}
  \end{equation}
\end{thm}

\input{relax-thm.tkz}


Скорость релаксации может быть оценена с использованием следующего
соотношения:
\begin{equation}
  \label{eq:relax-speed}
  2\abs{f(x^{k+1})-f(x^k)}=\sum_{\lambda_i^+>0} \left\{\xi_{i,k}^2
    \lambda_i^+ [1-R^2(\lambda_i^+)]\right\} + \sum_{\lambda_i^-<0} \left\{\xi_{i,k}^2
    \abs{\lambda_i^-} [R^2(\lambda_i^-)-1]\right\}
\end{equation}
здесь $\lambda_i^+$ и $\lambda_i^-$ — положительные и отрицательные
собственные значения матрицы $G_k$. Коэффициенты $\xi_{i,k}$
происходят из разложения $x^k=\sum_i{\xi_{i,k}u^i}$ по собственным
векторам $u^i$ матрицы Гессе.

Таким образом, эффективными оказываются методы, функция релаксации
которых в положительной области значений $\lambda$ как можно
\emph{меньше} уклоняется от нуля, а при отрицательных $\lambda$ —
становится как можно большей по модулю.

В качестве примера рассмотрим метод \neword{простого градиентного
  спуска} (ПГС), рабочая формула которого имеет вид:
\begin{equation*}
  x^{k+1}=x^k-hg_k,\, h\in\set{R}
\end{equation*}
Рассмотрим функцию релаксации метода ПГС:
\begin{equation*}
  R(\lambda) = 1 - \lambda h
\end{equation*}

\input{gd-relax.tkz}

Из её графика на рисунке \ref{fig:relax-thm} видно, что метод применим
лишь в том случае, когда собственные значения матрицы Гессе
оптимизируемой функции не превосходят некоторого критического значения
$M*=\frac{2}{h}$. Кроме того, в области малых значений $\lambda$
согласно \eqref{eq:relax-speed} скорость релаксации сильно снижается.

\subsection{Примеры работы}
Одним из классических тестов для различных алгоритмов оптимизации
является тест Розенброка, заключающийся в минимизации следующей
функции:
\begin{equation}
  \label{eq:rosenbrock}
  f(x, y) = 100(y - x²)² + (1 - x)²
\end{equation}
Функция Розенброка имеет глубокую впадину вдоль кривой $y=x^2$, что
обуславливает плохую сходимость простых методов.

\pgfplotsset{every axis grid/.append style={densely dashed}}
\begin{figure}[hb]
  \centering
  \begin{tikzpicture}[scale=1]
    \begin{axis}[xlabel=$x$, ylabel=$y$,grid]
      \input{rosenbrock-contours.tkz.tex}
      \input{rosenbrock_relch_-1.2,1_50-trace.tkz.tex}
    \end{axis}
  \end{tikzpicture}
  \caption{Работа алгоритма с функцией Розенброка
    \eqref{eq:rosenbrock}}
\end{figure}

Теперь рассмотрим функцию
\begin{equation}
  \label{eq:exptest}
  f(x, y) = \sum\limits_{a=\overline{0.1, 1.0}}\left [
    e^{-xa}-e^{-ya}-(e^{-a}-e^{-10a})\right ]^2
\end{equation}
Здесь суммирование происходит по значениям $a = 0.1, 0.2, \dotsc 1$.

\begin{figure}[hb]
  \centering
  \begin{tikzpicture}[scale=1]
    \begin{axis}[xlabel=$x$, ylabel=$y$,grid]
      \input{exptest-contours.tkz.tex}
      \input{exptest_relch_-0.5,1.5_20-trace.tkz.tex}
      \input{exptest_relch_5,5_20-trace.tkz.tex}
      \input{exptest_relch_3,-1_20-trace.tkz.tex}
    \end{axis}
  \end{tikzpicture}
  \caption{Линии уровня и трассировка процесса минимизации функции
    \eqref{eq:exptest}}
\end{figure}

В следующем примере рассмотрим функцию Химмельблау, которая задаётся
следующим образом:
\begin{equation}
  \label{eq:himmelblau}
  f(x, y) = (x² + y - 11)² + (x + y² - 7)²
\end{equation}
Известно, что локальные минимумы функции \eqref{eq:himmelblau}
расположены в точках $A=(-3.78, -3.28)$, $B=(-2.80, 3.13)$, $C=(3.58,
-1.85)$, $D=(3.00, 2.00)$, и в каждой из них достигается значение $0$.
Этим обусловлена следующая особенность работы алгоритма — результат
значительно зависит от выбора начального приближения, что
продемонстрировано на \ref{fig:himmelblau}.

\begin{figure}[hb]
  \label{fig:himmelblau}
  \centering
  \begin{tikzpicture}[scale=1]
    \begin{axis}[xlabel=$x$, ylabel=$y$,grid]
      \input{himmelblau-contours.tkz.tex}
      \input{himmelblau_relch_0.1,0.1_10-trace.tkz.tex}
      \input{himmelblau_relch_0.5,-0.5_10-trace.tkz.tex}
      \input{himmelblau_relch_-0.5,0.5_10-trace.tkz.tex}
      \input{himmelblau_relch_-0.5,-0.5_10-trace.tkz.tex}
    \end{axis}      
  \end{tikzpicture}
  \caption{Результаты работы алгоритма с функцией Химмельблау в
    зависимости от выбора начальной точки}
\end{figure}

\clearpage
\appendix
\bibliographystyle{gost71s}
\bibliography{paper}

\end{document}
