\subsection{Задача оптимизации и связанные с ней проблемы}

Необходимость поиска экстремальных значений разнообразных функций
встречается в практической деятельности повсеместно. В связи с этим
разработка и исследование эффективных алгоритмов оптимизации, имеющих
широкие области применения, является одним из важных направлений
современной математики.

Обратимся к следующей общей формулировке задачи оптимизации:
\begin{equation}
  \label{eq:optim-problem}
  f(x) \to \min
\end{equation}
В данной работе рассматривается оптимизация действительных функций
действительного переменного, $f\colon \set{R}^n \to \set{R}$.

Задача \eqref{eq:optim-problem}, где на возможные значения $x$ не
накладывается никаких ограничений, называется задачей
\neword{безусловной оптимизации}. В то же время, на практике
приходится решать и задачи \neword{условной оптимизации}:
\begin{equation}
  \label{eq:optim-problem-c}
  f(x) \to \min\, \text{при } x \in D
\end{equation}
где $D$ является некоторым подмножеством области определения $f(x)$.

\begin{rem}
  Рассмотрение задачи на поиск \emph{минимума} в качестве
  оптимизационной не ограничивает общности рассуждений в том смысле,
  что задача \neword{максимизации} функции $f(x)$
  \begin{equation*}
    f(x) \to \max
  \end{equation*}
  сводится к задаче минимизации противоположной функции
  $-f(x)$.\footnote[1]{Сходные соображения бывают справедливы и для
    ограничения \mbox{$x \in D$}, см. далее замечания \ref{rem:slack},
    \ref{rem:eq-noneq}}
\end{rem}
\subsection{Проблемы, возникающие в задачах оптимизации}
Обозначим трудности, проявляющиеся в ходе решения различных
экстремальных задач.

\subsubsection{Явление овражности}
\label{sec:problems-ill}

Большое число различных методов оптимизационных тем или иным образом
используют тот факт, что антиградиент функции $-f'(x)$ указывает на
направление её наискорейшего убывания.

Метод простого градиентного спуска\footnote{\gd{} более подробно
  рассмотрен далее, см. раздел \ref{sec:gd}} (\gd{}) реализует эту
идею самым простым образом, а именно: поиск минимума функции
осуществляется путём последовательного продвижения вдоль её
антиградиента. На рисунке \ref{fig:gd-works} демонстрируется, как
\gd{} находит локальный минимум функции\footnote{Так называемая
  функция Химмельблау также рассмотрена в
  разделе \ref{sec:himmelblau}}.

\begin{figure}[!thb]
  \centering
  \begin{tikzpicture}
    \begin{axis}[x=.5cm,y=.5cm]
      \input{himmelblau-contours.tkz.tex}
      \input{himmelblau_sgd_0.1,0.1_20_0.01-trace.tkz.tex}
      
      \node[circle,fill=black,scale=0.5,label={above
        right:\contour{white}{$(3,2)$}}] at (axis cs:3,2) {};
    \end{axis}
  \end{tikzpicture}
  \caption{Процесс поиска минимума функции}
  \label{fig:gd-works}
\end{figure}

Тем не менее, на практике нередки случаи, когда следование направлению
$-f'(x)$ почти никак не позволяет приблизиться к минимуму целевой
функции.

Подобная ситуация изображена на рисунке \ref{fig:gd-stalls}. Линии
уровня целевой функции сильно вытянуты, так что последовательные шаги
метода \gd{} постоянно оказываются то по одну, то по другую сторону
получившегося «оврага», а процесс поиска минимума сильно замедляется.

\begin{figure}[!thb]
  \centering
  \begin{tikzpicture}
    \begin{axis}[x=1cm,y=5cm]
      \input{gully-contours.tkz.tex}
      \input{gully_sgd_0.5,3_30_0.0000099-trace.tkz.tex}
      
%      \node[circle,fill=black,scale=0.5,label={below
%        left:\contour{white}{$(0.29,0.28)$}}] 
%      at (axis cs:0.285850, 0.279324) {};
    \end{axis}
  \end{tikzpicture}
  \caption[Овражная функция]{Зацикливание \gd{} на овражной функции}
  \label{fig:gd-works}
\end{figure}

В таком случае говорят об \neword{овражности} или \neword{плохой
  обусловленности} целевой функции. Аналогия с плохо обусловленной
матрицей не случайна. Связь понятий раскрывается в следующем
определении, которое вводит численную характеристику овражности
функции.

\begin{dfn}
  \label{dfn:ill-cond}
  \neword{Степенью овражности} выпуклой функции $f(x)$ называют
  отношение
  \begin{equation*}
    \eta = \frac{\lambda_{\max}}{\lambda_{\min}}
  \end{equation*}
  где $\lambda_{\max}, \lambda_{\min}$ — максимальное и минимальное
  собственное значение матрицы Гессе этой функции, соответственно.
\end{dfn}

Овражная функция имеет $\eta \gg 1$.

Степень овражности, грубо говоря, характеризует разброс собственных
значений матрицы Гессе. 

Как будет показано в разделе \ref{sec:relax}, спектр гессиана целевой
функции умеет весьма большое значение при анализе областей
применимости различных методов оптимизации.

\subsubsection{Явление многоэкстремальности}
