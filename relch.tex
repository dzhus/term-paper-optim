\section{Многопараметрическая оптимизация\\
  с чебышёвскими функциями релаксации}
\label{sec:relch}

В этом разделе представлена общая схема градиентных методов,
рассмотрено понятие функции релаксации и описан метод
многопараметрической оптимизации с чебышёвскими функциями релаксации
\relch{}, предложенный И. Г. Черноруцким в
\cite{chernorutsky04}.

\subsection{Теоретические сведения}

\subsubsection{Постановка задачи}

Настоящий раздел посвящён решению следующей задачи \emph{безусловной}
минимизации:
\begin{equation}
  \label{eq:optim-problem-form}
  f(x) \to \min,\quad x \in \set{R}^n,\, f(x) \in C^2(\set{R}^n)
\end{equation}

Рассматриваемые итерационных методов построены по общей схеме,
выражаемой следующей рабочей формулой, которая определяет способ
перехода к новому приближению $x^{k+1}$ точки минимума на очередной
итерации:
\begin{equation}
  \label{eq:iter-method}
  x^{k+1} = \phi_k(x^k)
\end{equation}

При этом используются следующие критерии останова процесса
\eqref{eq:iter-method} на $k$-м шаге:
\begin{enumerate}
\item Близость к нулю\footnote[1]{Здесь и в дальнейшем, $\epsilon>0$ —
    некоторое наперёд заданное малое число} нормы градиента функции:
  \begin{equation*}
  \norm{f'(x^k)} < \epsilon
\end{equation*}

\item Близость соседних приближений:
  \begin{equation*}
    \norm{x^{k+1}-x^k} < \epsilon
  \end{equation*}
  
\item Близость значений целевой функции $f(x)$ в точках соседних
  приближений:
  \begin{equation*}
    \norm{f(x^{k+1}) - f(x^k)} < \epsilon
  \end{equation*}
\item Остановка процесса после выполнения предельного количества
  итераций:
  \begin{equation*}
    k = k_{\max}
  \end{equation*}
  
\end{enumerate}

\subsubsection{Градиентные методы}

\begin{dfn}
  \neword{Градиентными} называются итерационные методы оптимизации со
  следующей рабочей формулой:
  \begin{equation}
    \label{eq:grad-methods}
    x^{k+1} = x^k - H_k\left(G_k, h_k\right) g_k
  \end{equation}
  здесь $H_k$ — некоторая функция от матрицы Гессе $G_k = G(x^k) =
  f''(x^k)$ и параметра $h_k$, а $g_k = g(x^k) = f'(x^k)$ — градиент
  функции в точке $x_k$.
\end{dfn}

При постоянных значениях параметров на различных шагах итерации
соответствующие индексы $k$ в формуле \eqref{eq:grad-methods}
опускаются.

Предполагается, что в некоторой $\epsilon_k$-окрестности $\left\{
  \norm{x-x^k} < \epsilon_k\right\}$ точки $x^k$ функция $f(x)$
приближается гиперболоидом:
\begin{equation}
  \label{eq:sqr-approx}
  f(x) \approx \frac{1}{2}\scalmult{G_k x, x} - \scalmult{a_k,x} + b_k \approx \frac{1}{2}\scalmult{G_k x, x}
\end{equation}

Ставится задача построения таких матричных функций $H_k$, при которых
выполняется \neword{условие релаксации} процесса
\begin{equation}
  \label{eq:relax-cond}
  f(x^{k+1}) < f(x^k)
\end{equation}
При этом требуется, чтобы величина нормы $\norm{x^{k+1}-x^{k}}$ была
ограничена сверху лишь параметром $\epsilon_k$, который характеризует
область справедливости локальной квадратичной модели
\eqref{eq:sqr-approx}.

\subsubsection{Функция релаксации}

\begin{dfn}
  \neword{Функцией релаксации} называется скалярная функция
  \begin{equation}
    \label{eq:relax-fun}
    R_h(\lambda) = 1 - H(\lambda, h)\lambda,\quad \lambda,h \in \set{R}
  \end{equation}
  где $H(\lambda, h)$ — скалярный аналог матричной функции $H(G, h)$
  из формулы \eqref{eq:grad-methods}.
\end{dfn}
В дальнейшем индекс $h$ у функции релаксации $R_h(\lambda)$ иногда
будем опускать.

\begin{dfn}
  \neword{Множителями релаксации} для точки $x^k$ называются значения
  функции релаксации на спектре матрицы Гессе:
  \begin{equation}
    \label{eq:relax-fac}
    R_h(\lambda_i),\, \lambda_i \in \Sp{G_k}
  \end{equation}
\end{dfn}

Благодаря следующей теореме, функция релаксации используется для
анализа различных градиентных методов.

\begin{thm}
  \label{thm:relax-thm}
  При любых $x^k$ для выполнения условия релаксации
  \eqref{eq:relax-cond} необходимо и достаточно, чтобы
  \begin{equation}
    \label{eq:relax-thm}
    \begin{aligned}
      & \abs{R(\lambda_i)} \geq 1 & \lambda_i& < 0 \\
      & \abs{R(\lambda_i)} \leq 1 & \lambda_i& > 0\\
      &&i& = \overline{1, n}
    \end{aligned}
  \end{equation}
\end{thm}

\input{relax-thm.tkz}

Скорость релаксации может быть оценена с использованием следующего
соотношения:
\begin{equation}
  \label{eq:relax-speed}
  2\abs{f(x^{k+1})-f(x^k)}=\sum_{\lambda_i^+>0} \left\{\xi_{i,k}^2
    \lambda_i^+ [1-R^2(\lambda_i^+)]\right\} + \sum_{\lambda_i^-<0} \left\{\xi_{i,k}^2
    \abs{\lambda_i^-} [R^2(\lambda_i^-)-1]\right\}
\end{equation}
здесь $\lambda_i^+$ и $\lambda_i^-$ — положительные и отрицательные
собственные значения матрицы $G_k$. Коэффициенты $\xi_{i,k}$
происходят из разложения $x^k=\sum_i{\xi_{i,k}u^i}$ по собственным
векторам $u^i$ матрицы Гессе.

Таким образом, эффективными оказываются методы, функция релаксации
которых в положительной области значений $\lambda$ как можно
\emph{меньше} уклоняется от нуля, а при отрицательных $\lambda$ —
становится как можно большей по модулю.

\subsubsection{Метод \gd{}}
\label{sec:gd}

В качестве примера рассмотрим метод \neword{простого градиентного
  спуска} (\gd{}), рабочая формула которого имеет вид:
\begin{equation}
  \label{eq:gd-workhorse}
  x^{k+1}=x^k-hg_k,\, h\in\set{R}
\end{equation}
Рассмотрим функцию релаксации метода \gd{}:
\begin{equation}
  \label{eq:gd-relax}
  R(\lambda) = 1 - \lambda h
\end{equation}

\input{gd-relax.tkz}

Из её графика на рисунке \ref{fig:gd-relax} видно, что метод применим
лишь в том случае, когда собственные значения матрицы Гессе
оптимизируемой функции не превосходят некоторого критического значения
$M^*=\frac{2}{h}$. Кроме того, в области значений $\lambda$, близких к
нулю или $M^*$, согласно \eqref{eq:relax-speed} скорость релаксации
сильно снижается.

\subsection{Описание метода \relch{}}

\subsubsection{Использование полиномов Чебышёва\\
  в функции релаксации}

Рассмотрим смещённые полиномы Чебышёва второго рода, которые
определяются согласно рекуррентному соотношению:
\begin{align}
  \label{eq:chebyshev}
  P_0(\lambda) &= 0 \\
  P_1(\lambda) &= 1 \\
  P_k(\lambda) &= 2(1-2\lambda)P_{k-1}(\lambda) - P_{k-2}(\lambda),\, k
  \geq 2
\end{align}

Функциональная последовательность таких полиномов обладает важным
свойством, а именно
\begin{equation}
  \label{eq:cheb-limit}
  \lim_{s\to\infty}{\frac{P_s(\lambda)}{s}} \to 0 \text{ на } (0;1)
\end{equation}
причём сходимость равномерная.

Предположим, что собственные числа матрицы Гессе целевой функции
$f(x)$ задачи \eqref{eq:optim-problem-form} в положительной области
спектра не превосходят 1 (для этого достаточно в рассмотрении считать
градиент и матрицу Гессе нормированными). Тогда использование
следующей функции в качестве релаксационной:
\begin{equation}
  \label{eq:cheb-relax}
  R_s(\lambda) = \frac{P_s(\lambda)}{s}
\end{equation}
позволяет обеспечить, согласно \eqref{eq:relax-speed}, сколь угодно
быструю релаксацию. На иллюстрации \ref{fig:cheb-relax} приведены
графики функции $R_s(\lambda)$ вблизи $[0;1]$ для нескольких значений
$s$.

\input{cheb-relax.tkz}

Уже при $s=8$ значение $R_s(\lambda)$ не превосходит по модулю $0.23$
на отрезке $[0.025; 0.975]$, обеспечивая хорошее подавление слагаемых
\eqref{eq:relax-speed}, соответствующих большому диапазону
положительных собственных чисел.

При этом стремление $R_s(\lambda)$ к $+\infty$ в
отрицательной части спектра также соответствует требованиям к хорошей
функции релаксации.

\subsubsection{Реализация метода}
Согласно \eqref{eq:relax-fun}, выбранной функции релаксации
соответствует зависимость
\begin{equation*}
  H(\lambda) = \frac{1-\frac{P_s(\lambda)}{s}}{\lambda}
\end{equation*}
откуда с учётом \eqref{eq:chebyshev} получим следующие рекуррентные
соотношения уже для функции $H(\lambda)$:
\begin{equation}
  \label{eq:cheb-scalarfun}
  \begin{aligned}
    H_1 &= 0 \\
    H_2 &= 2 \\
    H_{s+1} \mul (s+1) &= 2s\mul(1-2\lambda)\mul H_s-(s-1)\mul
    H_{s-1}+4s
  \end{aligned}
\end{equation}
где $s$ — параметр метода, равный степени используемых полиномов
Чебышёва. Соображения по выбору $s$ приведены ниже.

Подстановка полученного соотношения \eqref{eq:cheb-scalarfun}
в \eqref{eq:grad-methods} даёт следующую рабочую форумулу
\begin{multline}
  x^{k+1} = x^k - H_{s+1}g_k =\\=
  x^k-\frac{2s}{s+1}(E-2G_k)H_sg_k+\frac{s-1}{s+1}H_{s-1}g_k-\frac{4s}{s+1}g_k
\end{multline}

Таким образом, вектор смещения $d_{s+1} = x^{k+1} - x^k$ при выбранном
значении параметра $s$ на каждом шаге $k$ вычисляется по следующей
рекуррентной формуле:
\begin{equation}
  \label{eq:cheb-workhorse}
  \begin{aligned}
    d_1 &= 0\\
    d_2 &= -2g_k \\
    d_{s+1} &=
    \frac{2s}{s+1}(E-2G_k)d_{s}-\frac{s-1}{s+1}d_{s-1}-\frac{4s}{s+1}g_k
  \end{aligned}
\end{equation}

\subsubsection{Выбор параметра метода}
\label{sec:cheb-param}

Для применения на практике рассматриваемый метод минимизации требует
задания параметра $s$. Обратимся к способам выбора $s$.

Автором метода предлагается выбирать значение $s$ из следующего
соотношения:
\begin{equation}
  \eqref{eq:cheb-param}
  s = 1.3 \sqrt{\eta}
\end{equation}
где $\eta$ — оценка овражности минимизируемой функции. Таким образом,
возникает необходимость определения $\eta$ перед применением метода.
Для этого предлагаются следующие способы:
\begin{enumerate}
\item Вычисление степени овражности непосредственно из соотношения
  $\eta=\frac{\lambda_{\max}}{\lambda_{\min}}$ (см. определение
  \ref{dfn:ill-cond}). В случае задач малой размерности поиск
  собственных значений матрицы Гессе может быть осуществлён вручную,
  после чего найденное значение $\eta$ используется для выбора
  параметра $s$. В то же время при алгоритмическом построении набора
  собственных значений и делении на $\lambda_{\min}$ могут возникнуть
  проблемы численного переполнения, вызванные нехваткой машинной
  точности.
\item Предварительное использование метода \gd{} (см. раздел
  \ref{sec:gd}) для оценки $\eta$. Запускается решение исходной задачи
  минимизации \eqref{eq:optim-problem-form} с помощью \gd{}. Когда метод
  простого градиентного спуска зацикливается (см.
  \ref{sec:problems-ill}), отношение норм градиентов целевой функции в
  точках соседних приближений стабилизируется около некоторого $\mu$:
  \begin{equation*}
    \frac{\norm{f'(x^{k+1})}}{\norm{f'(x^k)}} \approx \mu
  \end{equation*}
  Тогда оценка овражности может быть найдена из следующего
  соотношения:
  \begin{equation*}
    \eta = \frac{2}{\abs{1-\mu}}
  \end{equation*}
  После этого из \eqref{eq:cheb-param} определяется $s$, и метод
  \relch{} начинает работу из последней найденной методом \gd{} точки
  приближения.

  Отметим, что в случае функции малой овражности достаточно хорошее
  решение может получиться уже на этапе применения \gd{}.
  
  Такой подход удобен и тем, что позволяет достаточно быстро
  «спуститься» в возможный овраг целевой функции благодаря невысокой
  вычислительной сложности рабочей формулы \eqref{eq:gd-workhorse}
  метода \gd{}, а в ряде случаев и найти минимум функции, совсем не
  прибегая к использованию метода \relch{}.
\end{enumerate}

При алгоритмическом построении $s$ нужно учитывать возможность
получения столь большого значения параметра, что на вычисления будет
затрачиваться непозволительно долгое время. Поэтому при начальном
рассмотрении задачи может потребоваться искусственное ограничение
параметра $s$ сверху.

\begin{rem}
  \label{rem:cheb-rel-speed}
  При увеличении параметра $s$ на некоторое ограниченное значение рост
  скорости релаксации, вообще говоря, не гарантируется.
\end{rem}

\clearpage
\subsection{Тестовые задачи}

В данном разделе рассмотрено несколько тестовых функций с их анализом
на выпуклость и овражность, а также приведены результаты применения
метода \relch{} для решения задач их минимизации.

\pgfplotsset{every axis/.append style={xlabel=$x$, ylabel=$y$,grid}}
\subsubsection{Функция Розенброка}

Одним из классических тестов для различных алгоритмов оптимизации
является тест Розенброка, заключающийся в минимизации следующей
функции:
\begin{equation}
  \label{eq:rosenbrock}
  f(x, y) = 100(y - x²)² + (1 - x)²
\end{equation}
Точку $(-1.2, 1)$ предлагается взять в качестве начального
приближения.

\paragraph{Выпуклость}

Матрица Гессе этой функции имеет вид:
\begin{equation}
  \label{eq:rosenbrock-hess}
  \begin{pmatrix}
    1200x^2-400y+2 & -400x \\
    -400x & \phm200\phantom{x}
  \end{pmatrix}
\end{equation}

Воспользуемся теоремой \ref{th:convex-f-hess} для проверки функции на
выпуклость. Применим критерий Сильвестра к матрице
\eqref{eq:rosenbrock-hess}. Запишем условия положительности её угловых
миноров:
\begin{align*}
  &\Delta_1 = 1200\,x^2-400\,y+2 > 0 \iff y < 3x^2+\frac{1}{200}\\
  &\Delta_2 = 80\,000\,x^2-80\,000\,y+400 > 0 \iff y < x^2+\frac{1}{200}
\end{align*}
При этом второе ограничение является более сильным. Таким образом,
граница области выпуклости функции Розенброка проходит вдоль кривой
\begin{equation}
  \label{eq:rosenbrock-convex-bound}
  y = x^2 +\frac{1}{200}
\end{equation}
Ниже этой кривой функция Розенброка строго выпукла. Согласно теореме
\ref{th:convex-f-smin}, она имеет в своей области выпуклости
единственную точку минимума.

При значениях $(x, y)$, лежащих выше кривой
\eqref{eq:rosenbrock-convex-bound}, функция Розенброка вогнута, а
потому, в силу теоремы \ref{th:convex-f-nomax}, не может иметь внутри
этой области точек минимума.

\paragraph{Овражность}

Собственные числа матрицы Гессе функции Розенброка в точке $(x, y)$
имеют следующие значения, отличающиеся лишь знаком перед корнем:
\begin{gather*}
  \begin{split}
    \lambda_1(x, y) = -\sqrt{400\,000\,y^2 +
      \left(39\,600-240\,000\,x^2\right)y + 360\,000\,x^4 +
      41\,200\,x^2+9801}- \\
    - 200y+600x^2+101
  \end{split}\\
  \begin{split}
    \lambda_2(x, y) = \sqrt{400\,000\,y^2 +
      \left(39\,600-240\,000\,x^2\right)y + 360\,000\,x^4 +
      41\,200\,x^2+9801}- \\
    - 200y+600x^2+101
  \end{split}
\end{gather*}

В точке начального приближения собственные значения матрицы Гессе
функции Розенброка приблизительно равны $23.63$ и $1506.37$, откуда
согласно \eqref{eq:gully} получается степень овражности
\begin{equation}
  \label{eq:rosenbrock-gully-start}
  \eta = \frac{1506.37}{23.63} \approx 63.7
\end{equation}

Исследуемая функция имеет кривую перегиба $y=x^2+\frac{1}{200}$,
разделяющую её области выпуклости и вогнутости. Вдоль этой кривой
степень овражности функции $\eta = \infty$.

\paragraph{Аналитическое решение задачи}

Найдём минимум функции \eqref{eq:rosenbrock} аналитически.

Из необходимых условий стационарности точки имеем:
\begin{align*}
  \pardiff{f}{x} &= 400x^3-400yx+2x-2 = 0 \\
  \pardiff{f}{y} &= 200y-200x^2 = 0
\end{align*}
откуда $x = y = 1$. Найденная точка $A(1, 1)$ не принадлежит кривой
перегиба $y = x^2+\frac{1}{200}$, а потому необходимые условия
экстремума для неё оказываются достаточными. Таким образом, с учётом
теорем \ref{th:convex-f-smin} и \ref{th:convex-f-nomax} точка $A(1, 1)$
является глобальным минимумом функции Розенброка.

Вычисление значения $\eta$ в $A$ даёт
\begin{equation}
  \label{eq:rosenbrock-gully-extr}
  \eta = \frac{1001.6006}{0.3994} \approx 2507.7631
\end{equation}

\paragraph{Ручной выбор параметра алгоритма \relch{}}

На иллюстрации \ref{fig:rosenbrock-contours} представлены линии уровня
функции Розенброка, которые сильно вытягиваются вдоль всей кривой
$y=x^2$ за счёт близости перегиба и быстрого роста функции в выпуклой
области. С учётом наблюдаемой картины, а также перепада в значениях
степени овражности \eqref{eq:rosenbrock-gully-start} и
\eqref{eq:rosenbrock-gully-extr} можно утверждать, что в ходе работы
алгоритму придётся бороться с сильной овражностью целевой функции.

\begin{figure}[!thb]
  \centering
  \begin{tikzpicture}
    \begin{axis}
      [x=3cm, y=2.7cm,
      xmin=-1.5, xmax=1.5,
      ymin=-1,ymax=2]
      \input{rosenbrock_hq-contours.tkz.tex}
      \addplot[mark=none,black,densely dashed] plot[domain=-1.41:1.41] function{x**2};
      \node[circle,fill=black,scale=0.5,label={right:\contour{white}{$A$}}] at
      (axis cs:1,1) {};
    \end{axis}
  \end{tikzpicture}
  \caption[Функция Розенброка]{Линии уровней $1, 2, 2.5, 4, 5, 50,
    100$ функции Розенброка \eqref{eq:rosenbrock}, её кривая перегиба
    $y=x^2+\frac{1}{200}$ и глобальный минимум в точке $A(1, 1)$}
  \label{fig:rosenbrock-contours}
\end{figure}

В качестве пробных оценок $\eta$ для применения в формуле
\eqref{eq:cheb-param} выберем значения
\eqref{eq:rosenbrock-gully-start} и \eqref{eq:rosenbrock-gully-extr}, тогда:
\begin{gather*}
  s_1 = 1.3 \sqrt{64} \approx 10 \\
  s_2 = 1.3 \sqrt{2507} \approx 66
\end{gather*}
Из эмпирических соображений в качестве $s$ выбиралось ближайшее к
$1.3\sqrt{\eta}$ \emph{чётное} число (в ходе тестов алгоритм
демонстрировал наилучшую сходимость именно при чётных $s$).

\begin{figure}[!thb]
  \centering
  \begin{tikzpicture}
    \begin{axis}[x=2.2cm, y=1.8cm]
      \input{rosenbrock-contours.tkz.tex}
      \input{rosenbrock_relch_-1.2,1_12_200-trace.tkz.tex}
        \node[circle,fill=black,scale=0.25,pin={below right:\contour{white}{$A$}}] at
        (axis cs:1,1) {};
    \end{axis}
  \end{tikzpicture}
  \caption[\relch{} на функции Розенброка, $s=200$]{Минимизация
    функции Розенброка алгоритмом \relch{} при $s=200$}
\end{figure}


\subsubsection{Экспоненциальная функция}

Теперь рассмотрим функцию
\begin{equation}
  \label{eq:exptest}
  f(x, y) = \sum\limits_{a=\overline{0.1, 1.0}}\left [
    e^{-xa}-e^{-ya}-(e^{-a}-e^{-10a})\right ]^2
\end{equation}
Здесь суммирование происходит по значениям $a = 0.1, 0.2, \dotsc, 1$.

\begin{figure}[!thb]
  \centering
  \begin{tikzpicture}
    \begin{axis}[x=1cm,y=1cm]
      \input{exptest-contours.tkz.tex}
      \input{exptest_relch_-0.5,1.5_20-trace.tkz.tex}
      \input{exptest_relch_5,5_20-trace.tkz.tex}
      \input{exptest_relch_3,-1_25_12-trace.tkz.tex}
      \node[circle,fill=black,scale=0.5,label=right:$A$] at (axis cs:1,10) {};
    \end{axis}
  \end{tikzpicture}
  \caption[Экспоненциальная функция]{Линии уровня и трассировка процесса минимизации функции
    \eqref{eq:exptest}}
\end{figure}

Алгоритм стабильно определяет минимум в точке $(1, 10)$.

\subsubsection{Функция Химмельблау}

В следующем примере рассмотрим функцию Химмельблау, которая задаётся
следующим образом:
\begin{equation}
  \label{eq:himmelblau}
  f(x, y) = (x² + y - 11)² + (x + y² - 7)²
\end{equation}
Известно, что локальные минимумы функции \eqref{eq:himmelblau}
расположены в точках $A=(-3.78, -3.28)$, $B=(-2.80, 3.13)$, $C=(3.58,
-1.85)$, $D=(3.00, 2.00)$, и в каждой из них достигается значение $0$.
Этим обусловлена следующая особенность работы алгоритма — результат
значительно зависит от выбора начального приближения, что
продемонстрировано на рисунке \ref{fig:himmelblau}.

\begin{figure}[!thb]
  \centering
  \begin{tikzpicture}
    \begin{axis}[x=.8cm,y=.8cm]
      \input{himmelblau-contours.tkz.tex}
      \input{himmelblau_relch_0.1,0.1_10-trace.tkz.tex}
      \input{himmelblau_relch_0.5,-0.5_10-trace.tkz.tex}
      \input{himmelblau_relch_-0.5,0.5_10-trace.tkz.tex}
      \input{himmelblau_relch_-0.5,-0.5_10-trace.tkz.tex}

      \node[circle,fill=black,scale=0.5,label=above left:$A$] at (axis cs:-3.78,-3.28) {};
      \node[circle,fill=black,scale=0.5,label=below left:$B$] at (axis cs:-2.8,3.13) {};
      \node[circle,fill=black,scale=0.5,label=below right:$C$] at (axis cs:3.58,-1.85) {};
      \node[circle,fill=black,scale=0.5,label=above right:$D$] at (axis cs:3,2) {};
    \end{axis}      
  \end{tikzpicture}
  \caption[Функция Химмельблау]{Результаты работы алгоритма с функцией
    Химмельблау \eqref{eq:himmelblau} в зависимости от выбора
    начальной точки}
  \label{fig:himmelblau}
\end{figure}
