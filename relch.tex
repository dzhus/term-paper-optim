\section{Многопараметрическая оптимизация\\
  с чебышёвскими функциями релаксации}
\label{sec:relch}

В этом разделе представлена общая схема градиентных методов,
рассмотрено понятие функции релаксации и описан метод
многопараметрической оптимизации с чебышёвскими функциями релаксации
\relch{}, предложенный И. Г. Черноруцким в
\cite{chernorutsky04}.

\subsection{Теоретические сведения}

\subsubsection{Постановка задачи}

Настоящий раздел посвящён решению следующей задачи \emph{безусловной}
минимизации:
\begin{equation}
  \label{eq:optim-problem-form}
  f(x) \to \min,\quad x \in \set{R}^n,\, f(x) \in C^2(\set{R}^n)
\end{equation}

Рассматриваемые итерационных методов построены по общей схеме,
выражаемой следующей рабочей формулой, которая определяет способ
перехода к новому приближению $x^{k+1}$ точки минимума на очередной
итерации:
\begin{equation}
  \label{eq:iter-method}
  x^{k+1} = \phi_k(x^k)
\end{equation}

При этом используются следующие критерии останова процесса
\eqref{eq:iter-method} на $k$-м шаге:
\begin{enumerate}
\item Близость к нулю\footnote{Здесь и в дальнейшем, $\epsilon>0$ —
    некоторое наперёд заданное малое число.} нормы градиента функции:
  \begin{equation*}
  \norm{f'(x^k)} < \epsilon
\end{equation*}

\item Близость соседних приближений:
  \begin{equation*}
    \norm{x^{k+1}-x^k} < \epsilon
  \end{equation*}
  
\item Близость значений целевой функции $f(x)$ в точках соседних
  приближений:
  \begin{equation*}
    \norm{f(x^{k+1}) - f(x^k)} < \epsilon
  \end{equation*}
\item Остановка процесса после выполнения предельного количества
  итераций:
  \begin{equation*}
    k = k_{\max}
  \end{equation*}
  
\end{enumerate}

\subsubsection{Градиентные методы}

\begin{dfn}
  \neword{Градиентными} называются итерационные методы оптимизации со
  следующей рабочей формулой:
  \begin{equation}
    \label{eq:grad-methods}
    x^{k+1} = x^k - H_k\left(G_k, h_k\right) g_k
  \end{equation}
  здесь $H_k$ — некоторая функция от матрицы Гессе $G_k = G(x^k) =
  f''(x^k)$ и параметра $h_k$, а $g_k = g(x^k) = f'(x^k)$ — градиент
  функции в точке $x_k$.
\end{dfn}

При постоянных значениях параметров на различных шагах итерации
соответствующие индексы $k$ в формуле \eqref{eq:grad-methods}
опускаются.

Предполагается, что в некоторой $\epsilon_k$-окрестности $\left\{
  \norm{x-x^k} < \epsilon_k\right\}$ точки $x^k$ функция $f(x)$
приближается гиперболоидом:
\begin{equation}
  \label{eq:sqr-approx}
  f(x) \approx \frac{1}{2}\scalmult{G_k x, x} - \scalmult{a_k,x} + b_k \approx \frac{1}{2}\scalmult{G_k x, x}
\end{equation}

Ставится задача построения таких матричных функций $H_k$, при которых
выполняется \neword{условие релаксации} процесса
\begin{equation}
  \label{eq:relax-cond}
  f(x^{k+1}) < f(x^k)
\end{equation}
При этом требуется, чтобы величина нормы $\norm{x^{k+1}-x^{k}}$ была
ограничена сверху лишь параметром $\epsilon_k$, который характеризует
область справедливости локальной квадратичной модели
\eqref{eq:sqr-approx}.

\subsubsection{Функция релаксации}
\label{sec:relax}
\begin{dfn}
  \neword{Функцией релаксации} называется скалярная функция
  \begin{equation}
    \label{eq:relax-fun}
    R_h(\lambda) = 1 - H(\lambda, h)\lambda,\quad \lambda,h \in \set{R}
  \end{equation}
  где $H(\lambda, h)$ — скалярный аналог матричной функции $H(G, h)$
  из формулы \eqref{eq:grad-methods}.
\end{dfn}
В дальнейшем индекс $h$ у функции релаксации $R_h(\lambda)$ иногда
будем опускать.

\begin{dfn}
  \neword{Множителями релаксации} для точки $x^k$ называются значения
  функции релаксации на спектре матрицы Гессе:
  \begin{equation}
    \label{eq:relax-fac}
    R_h(\lambda_i),\, \lambda_i \in \Sp{G_k}
  \end{equation}
\end{dfn}

Благодаря следующей теореме, функция релаксации используется для
анализа различных градиентных методов.

\begin{thm}
  \label{thm:relax-thm}
  Для выполнения условия релаксации \eqref{eq:relax-cond} при любых
  $x^k$ необходимо и достаточно, чтобы
  \begin{equation}
    \label{eq:relax-thm}
    \begin{aligned}
      & \abs{R(\lambda_i)} \geq 1 & \lambda_i& < 0 \\
      & \abs{R(\lambda_i)} \leq 1 & \lambda_i& > 0\\
      &&i& = \overline{1, n}
    \end{aligned}
  \end{equation}
\end{thm}

\input{relax-thm.tkz}

Скорость релаксации может быть оценена с использованием следующего
соотношения:
\begin{equation}
  \label{eq:relax-speed}
  2\abs{f(x^{k+1})-f(x^k)}=\sum_{\lambda_i^+>0} \left\{\xi_{i,k}^2
    \lambda_i^+ [1-R^2(\lambda_i^+)]\right\} + \sum_{\lambda_i^-<0} \left\{\xi_{i,k}^2
    \abs{\lambda_i^-} [R^2(\lambda_i^-)-1]\right\}
\end{equation}
здесь $\lambda_i^+$ и $\lambda_i^-$ — положительные и отрицательные
собственные значения матрицы $G_k$. Коэффициенты $\xi_{i,k}$
происходят из разложения $x^k=\sum_i{\xi_{i,k}u^i}$ по собственным
векторам $u^i$ матрицы Гессе.

Таким образом, эффективными оказываются методы, функция релаксации
которых в положительной области значений $\lambda$ как можно
\emph{меньше} уклоняется от нуля, а при отрицательных $\lambda$ —
становится как можно большей по модулю.

\subsubsection{Метод \gd{}}
\label{sec:gd}

В качестве примера рассмотрим\footnote{Этот метод уже упоминался в
  разделе \ref{sec:problems-ill}.} метод \neword{простого градиентного
  спуска} (\gd{}), рабочая формула которого имеет вид:
\begin{equation}
  \label{eq:gd-workhorse}
  x^{k+1}=x^k-hg_k
\end{equation}
где $h\in\set{R}$ — некоторое фиксированное число (обычно малое),
называемое \neword{шагом}.

Рассмотрим функцию релаксации метода \gd{}:
\begin{equation}
  \label{eq:gd-relax}
  R(\lambda) = 1 - \lambda h
\end{equation}

\input{gd-relax.tkz}

Из её графика на рисунке \ref{fig:gd-relax} видно, что метод применим
лишь в том случае, когда собственные значения матрицы Гессе
оптимизируемой функции не превосходят некоторого критического значения
$M^*=\frac{2}{h}$. Кроме того, в области значений $\lambda$, близких к
нулю или $M^*$, согласно \eqref{eq:relax-speed} скорость релаксации
сильно снижается. 

Так объясняется не самая широкая область применимости \gd{} и проблемы
градиентного спуска, обозначенные в разделе \ref{sec:problems-ill}.

Имеются различные модификации метода \gd{}. Одной из наиболее простых
является метод \rgd{} — простой градиентный спуск \neword{с дроблением
  шага}. В \rgd{} изначально $h$ полагается достаточно большим, но в
случае нарушения условия релаксации \eqref{eq:relax-cond} на
какой-либо итерации $h$ делится пополам до тех пор, пока
\eqref{eq:relax-cond} вновь не станет выполняться.

\subsection{Описание метода \relch{}}

\subsubsection{Использование полиномов Чебышёва\\
  в функции релаксации}

Рассмотрим смещённые полиномы Чебышёва второго рода, которые
определяются согласно рекуррентному соотношению:
\begin{align}
  \label{eq:chebyshev}
  P_0(\lambda) &= 0 \\
  P_1(\lambda) &= 1 \\
  P_k(\lambda) &= 2(1-2\lambda)P_{k-1}(\lambda) - P_{k-2}(\lambda),\, k
  \geq 2
\end{align}

Функциональная последовательность таких полиномов обладает важным
свойством, а именно
\begin{equation}
  \label{eq:cheb-limit}
  \lim_{s\to\infty}{\frac{P_s(\lambda)}{s}} \to 0 \text{ на } (0;1)
\end{equation}
причём сходимость равномерная.

Предположим, что собственные числа матрицы Гессе целевой функции
$f(x)$ задачи \eqref{eq:optim-problem-form} в положительной области
спектра не превосходят 1 (для этого достаточно в рассмотрении считать
градиент и матрицу Гессе нормированными). Тогда использование
следующей функции в качестве релаксационной:
\begin{equation}
  \label{eq:cheb-relax}
  R_s(\lambda) = \frac{P_s(\lambda)}{s}
\end{equation}
позволяет обеспечить, согласно \eqref{eq:relax-speed}, сколь угодно
быструю релаксацию. На иллюстрации \ref{fig:cheb-relax} приведены
графики функции $R_s(\lambda)$ вблизи $[0;1]$ для нескольких значений
$s$.

\input{cheb-relax.tkz}

Уже при $s=8$ значение $R_s(\lambda)$ не превосходит по модулю $0.23$
на отрезке $[0.025; 0.975]$, обеспечивая хорошее подавление слагаемых
\eqref{eq:relax-speed}, соответствующих большому диапазону
положительных собственных чисел.

При этом стремление $R_s(\lambda)$ к $+\infty$ в
отрицательной части спектра также соответствует требованиям к хорошей
функции релаксации.

\subsubsection{Реализация метода}
Согласно \eqref{eq:relax-fun}, выбранной функции релаксации
соответствует зависимость
\begin{equation*}
  H(\lambda) = \frac{1-\frac{P_s(\lambda)}{s}}{\lambda}
\end{equation*}
откуда с учётом \eqref{eq:chebyshev} получим следующие рекуррентные
соотношения уже для функции $H(\lambda)$:
\begin{equation}
  \label{eq:cheb-scalarfun}
  \begin{aligned}
    H_1 &= 0 \\
    H_2 &= 2 \\
    H_{s+1} \mul (s+1) &= 2s\mul(1-2\lambda)\mul H_s-(s-1)\mul
    H_{s-1}+4s
  \end{aligned}
\end{equation}
где $s$ — параметр метода, равный степени используемых полиномов
Чебышёва. Соображения по выбору $s$ приведены ниже.

Подстановка полученного соотношения \eqref{eq:cheb-scalarfun}
в \eqref{eq:grad-methods} даёт следующую рабочую форумулу
\begin{multline}
  x^{k+1} = x^k - H_{s+1}g_k =\\=
  x^k-\frac{2s}{s+1}(E-2G_k)H_sg_k+\frac{s-1}{s+1}H_{s-1}g_k-\frac{4s}{s+1}g_k
\end{multline}

Таким образом, вектор смещения $d_{s+1} = x^{k+1} - x^k$ при выбранном
значении параметра $s$ на каждом шаге $k$ вычисляется по следующей
рекуррентной формуле:
\begin{equation}
  \label{eq:cheb-workhorse}
  \begin{aligned}
    d_1 &= 0\\
    d_2 &= -2g_k \\
    d_{s+1} &=
    \frac{2s}{s+1}(E-2G_k)d_{s}-\frac{s-1}{s+1}d_{s-1}-\frac{4s}{s+1}g_k
  \end{aligned}
\end{equation}

После вычисления $d_{s+1}$ применяется регулировка шага путём деления
его пополам:
\begin{equation}
  \label{eq:cheb-regulation}
  d_{s+1} = \frac{d_{s+1}}{2}
\end{equation}
причём регулировка \eqref{eq:cheb-regulation} последовательно
продолжается до тех пор, пока не будет обеспечено условие релаксации
\eqref{eq:relax-cond} процесса оптимизации. Подобное ограничение шага
$d_{s+1}$ используется в целях предотвращения выхода из области
справедливости локальной квадратичной модели \eqref{eq:sqr-approx}. На
идее такой регулировки построен метод \rgd{}, описанный ранее в
разделе \ref{sec:gd}.

\subsubsection{Выбор параметра метода}
\label{sec:cheb-param}

Для применения на практике рассматриваемый метод минимизации требует
задания параметра $s$. Обратимся к способам выбора $s$.

Автором метода предлагается выбирать значение $s$ из следующего
соотношения:
\begin{equation}
  \label{eq:cheb-param}
  s = 1.3 \sqrt{\eta}
\end{equation}
где $\eta$ — оценка овражности минимизируемой функции. Таким образом,
возникает необходимость определения $\eta$ перед применением метода.
Рассмотрим предлагаемые для этого способы.

\paragraph{Использование гессиана}

Можно определять степень овражности непосредственно из соотношения
$\eta=\frac{\lambda_{\max}}{\lambda_{\min}}$ (см.
определение \ref{dfn:ill-cond}), исследуя аналитические выражения для
собственных чисел гессиана функции или же вычисляя их непосредственно
численно. При алгоритмическом построении спектра матрцы Гессе и
делении на $\lambda_{\min}$ могут возникнуть проблемы численного
переполнения, вызванные нехваткой машинной точности.

\paragraph{Использование метода \gd{}}

Данный подход предусматривает применение метода простого градиентного
спуска (см. раздел \ref{sec:gd}) для оценки $\eta$. 

Для этого сначала запускается решение исходной задачи минимизации
\eqref{eq:optim-problem-form} с помощью \gd{}. Когда метод простого
градиентного спуска зацикливается (см. \ref{sec:problems-ill}),
отношение норм градиентов целевой функции в точках соседних
приближений стабилизируется около некоторого $\mu$:
\begin{equation}
  \label{eq:gdrelch-stabilization}
  \frac{\norm{f'(x^{k+1})}}{\norm{f'(x^k)}} \approx \mu
\end{equation}
Тогда оценка овражности может быть найдена из следующего
соотношения:
\begin{equation}
  \label{eq:gdrelch-param}
  \eta = \frac{2}{\abs{1-\mu}}
\end{equation}
После этого из \eqref{eq:cheb-param} определяется $s$, и метод
\relch{} начинает работу из последней найденной методом \gd{} точки
приближения.

Для точности оценки $s$ необходимо в качестве шага $h$ в рабочей
формуле \eqref{eq:gd-workhorse} метода \gd{} выбирать как можно
большее значение, при котором обеспечивается стабилизация отношения
\eqref{eq:gdrelch-stabilization}. При этом для определения $h$,
удовлетворяющего такому свойству, могут потребоваться предварительные
попытки решения задачи с различными пробными значениями $h$.

Отметим, что в случае функции малой овражности достаточно хорошее
решение может получиться уже на этапе применения \gd{}.

При алгоритмическом построении $s$ нужно учитывать возможность
получения столь большого значения параметра, что на вычисления будет
затрачиваться непозволительно долгое время. Поэтому при начальном
рассмотрении задачи может потребоваться искусственное ограничение
параметра $s$ сверху.

Двухэтапную модификацию метода \relch{} в дальнейшем будем обозначать
как \gdrelch{}.

\begin{rem}
  \label{rem:cheb-rel-speed}
  При увеличении параметра $s$ на некоторое ограниченное значение рост
  скорости релаксации, вообще говоря, не гарантируется.
\end{rem}

В разделе \ref{sec:test-problems} представлены результаты применения
\gdrelch{} на тестовых функциях.

\subsubsection{Трудности, возникающие при реализации \relch{}}

В ходе тестирования \relch{} были выявлены случаи, когда метод
попадает в такую точку, что норма градиента и значение гессиана
функции приводят к численному переполнению в результате многократных
вычислений по формуле \eqref{eq:cheb-workhorse}, так что условие
релаксации нарушается, несмотря на регулировку
\eqref{eq:cheb-regulation}.

Для борьбы с таким поведением использовалась следующая
\emph{эвристическая} техника: после вычисления
\eqref{eq:cheb-workhorse} при обнаружении бесконечного значения
$\norm{d_{s+1}}$ следующее приближение выбиралось в случайном
направлении на расстоянии $\sqrt{n}$ от текущего. В общем случае
невозможно доказать результатовность таких действий, однако подход
продемонстрировал успех на ряде тестовых задач.

При выборе начального приближения вдали от точки минимума многократные
вычисления по формуле \eqref{eq:cheb-workhorse} также могут привести к
численному переполнению и из-за большого по модулю значения целевой
функции. В определённых случаях проблемы можно избежать, используя на
первом этапе метод \gd{} или \rgd{} для начального снижения значения
целевого функционала.

\clearpage
\subsection{Тестовые задачи}
\label{sec:test-problems}

В данном разделе рассмотрено несколько тестовых функций с их анализом
на выпуклость и овражность, а также приведены результаты применения
метода \relch{} для решения задач их минимизации. Для анализа
выпуклости функций использовалась теорема \ref{th:convex-f-hess}.
Овражность оценивалась эвристически и на основе информации о
применимости метода \gd{} к целевой функции
(см. разделы \ref{sec:gd}, \ref{sec:problems-ill}).

\pgfplotsset{every axis/.append style={xlabel=$x$, ylabel=$y$}}

\input{rosenbrock.tex}

\clearpage
\input{himmelblau.tex}

\clearpage
\input{exptest.tex}

\clearpage
\input{hess-singular.tex}

\subsection{Выводы}

Описанный и протестированный алгоритм \relch{} обладает следующими
особенностями:
\begin{itemize}
\item Теоретически доказана бесконечная скорость релаксации \relch{}.
\item Метод \relch{} обладает устойчивостью к овражности целевого
  функционала.
\item Возможно алгоритмическое определение значения параметра $s$ с
  помощью предварительного использования метода \gd{}.
\item Метод относительно просто реализуется, в алгоритме нет
  концептуально сложных вычислений.
\item Автором метода в \cite{chernorutsky04} также указывается на
  возможность применения алгоритма для оптимизации
  многопараметрических систем, в которых гессиан целевого функционала
  представляет собой разреженную матрицу большой размерности.
\end{itemize}

В разделе \ref{sec:sources} представлены исходные тексты реализации
\relch{} и \gdrelch{} на языке Scheme.
